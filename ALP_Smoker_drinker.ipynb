{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Dum20jw3YGEQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Load Data"
      ],
      "metadata": {
        "id": "QwhGInV2hkyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from the provided link\n",
        "url = 'https://media.githubusercontent.com/media/Marsh16/smoking-drinking-ALP-ML/main/smoking_drinking_data.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "BmLElCfjYNj0",
        "outputId": "d3b5820b-a921-4e46-ddf0-aecdc8de1f5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 age         height         weight      waistline  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean       47.614491     162.240625      63.284050      81.233358   \n",
              "std        14.181339       9.282957      12.514241      11.850323   \n",
              "min        20.000000     130.000000      25.000000       8.000000   \n",
              "25%        35.000000     155.000000      55.000000      74.100000   \n",
              "50%        45.000000     160.000000      60.000000      81.000000   \n",
              "75%        60.000000     170.000000      70.000000      87.800000   \n",
              "max        85.000000     190.000000     140.000000     999.000000   \n",
              "\n",
              "          sight_left    sight_right      hear_left     hear_right  \\\n",
              "count  991346.000000  991346.000000  991346.000000  991346.000000   \n",
              "mean        0.980834       0.978429       1.031495       1.030476   \n",
              "std         0.605949       0.604774       0.174650       0.171892   \n",
              "min         0.100000       0.100000       1.000000       1.000000   \n",
              "25%         0.700000       0.700000       1.000000       1.000000   \n",
              "50%         1.000000       1.000000       1.000000       1.000000   \n",
              "75%         1.200000       1.200000       1.000000       1.000000   \n",
              "max         9.900000       9.900000       2.000000       2.000000   \n",
              "\n",
              "                 SBP            DBP  ...      HDL_chole      LDL_chole  \\\n",
              "count  991346.000000  991346.000000  ...  991346.000000  991346.000000   \n",
              "mean      122.432498      76.052627  ...      56.936800     113.037692   \n",
              "std        14.543148       9.889365  ...      17.238479      35.842812   \n",
              "min        67.000000      32.000000  ...       1.000000       1.000000   \n",
              "25%       112.000000      70.000000  ...      46.000000      89.000000   \n",
              "50%       120.000000      76.000000  ...      55.000000     111.000000   \n",
              "75%       131.000000      82.000000  ...      66.000000     135.000000   \n",
              "max       273.000000     185.000000  ...    8110.000000    5119.000000   \n",
              "\n",
              "        triglyceride     hemoglobin  urine_protein  serum_creatinine  \\\n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000   \n",
              "mean      132.141751      14.229824       1.094224          0.860467   \n",
              "std       102.196985       1.584929       0.437724          0.480530   \n",
              "min         1.000000       1.000000       1.000000          0.100000   \n",
              "25%        73.000000      13.200000       1.000000          0.700000   \n",
              "50%       106.000000      14.300000       1.000000          0.800000   \n",
              "75%       159.000000      15.400000       1.000000          1.000000   \n",
              "max      9490.000000      25.000000       6.000000         98.000000   \n",
              "\n",
              "            SGOT_AST       SGOT_ALT      gamma_GTP  SMK_stat_type_cd  \n",
              "count  991346.000000  991346.000000  991346.000000     991346.000000  \n",
              "mean       25.989308      25.755051      37.136347          1.608122  \n",
              "std        23.493386      26.308599      50.424153          0.818507  \n",
              "min         1.000000       1.000000       1.000000          1.000000  \n",
              "25%        19.000000      15.000000      16.000000          1.000000  \n",
              "50%        23.000000      20.000000      23.000000          1.000000  \n",
              "75%        28.000000      29.000000      39.000000          2.000000  \n",
              "max      9999.000000    7210.000000     999.000000          3.000000  \n",
              "\n",
              "[8 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0533cdff-1296-4e95-9d2b-97ce1ac5424e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>...</th>\n",
              "      <th>HDL_chole</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "      <td>991346.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>47.614491</td>\n",
              "      <td>162.240625</td>\n",
              "      <td>63.284050</td>\n",
              "      <td>81.233358</td>\n",
              "      <td>0.980834</td>\n",
              "      <td>0.978429</td>\n",
              "      <td>1.031495</td>\n",
              "      <td>1.030476</td>\n",
              "      <td>122.432498</td>\n",
              "      <td>76.052627</td>\n",
              "      <td>...</td>\n",
              "      <td>56.936800</td>\n",
              "      <td>113.037692</td>\n",
              "      <td>132.141751</td>\n",
              "      <td>14.229824</td>\n",
              "      <td>1.094224</td>\n",
              "      <td>0.860467</td>\n",
              "      <td>25.989308</td>\n",
              "      <td>25.755051</td>\n",
              "      <td>37.136347</td>\n",
              "      <td>1.608122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.181339</td>\n",
              "      <td>9.282957</td>\n",
              "      <td>12.514241</td>\n",
              "      <td>11.850323</td>\n",
              "      <td>0.605949</td>\n",
              "      <td>0.604774</td>\n",
              "      <td>0.174650</td>\n",
              "      <td>0.171892</td>\n",
              "      <td>14.543148</td>\n",
              "      <td>9.889365</td>\n",
              "      <td>...</td>\n",
              "      <td>17.238479</td>\n",
              "      <td>35.842812</td>\n",
              "      <td>102.196985</td>\n",
              "      <td>1.584929</td>\n",
              "      <td>0.437724</td>\n",
              "      <td>0.480530</td>\n",
              "      <td>23.493386</td>\n",
              "      <td>26.308599</td>\n",
              "      <td>50.424153</td>\n",
              "      <td>0.818507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>155.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>74.100000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>45.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>14.300000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>87.800000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>15.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>273.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>8110.000000</td>\n",
              "      <td>5119.000000</td>\n",
              "      <td>9490.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>9999.000000</td>\n",
              "      <td>7210.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0533cdff-1296-4e95-9d2b-97ce1ac5424e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0533cdff-1296-4e95-9d2b-97ce1ac5424e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0533cdff-1296-4e95-9d2b-97ce1ac5424e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eff5fd2f-30e6-4e17-ac1b-cf54d84dae8f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eff5fd2f-30e6-4e17-ac1b-cf54d84dae8f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eff5fd2f-30e6-4e17-ac1b-cf54d84dae8f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Preprocessing"
      ],
      "metadata": {
        "id": "gNlHPt95hn0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "df = df.where((pd.notnull(df)), '')  # Handling missing values\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKo7U5qjYPym",
        "outputId": "cd69f2e8-dd24-4ef4-829e-96162498237b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 991346 entries, 0 to 991345\n",
            "Data columns (total 24 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   sex               991346 non-null  object \n",
            " 1   age               991346 non-null  int64  \n",
            " 2   height            991346 non-null  int64  \n",
            " 3   weight            991346 non-null  int64  \n",
            " 4   waistline         991346 non-null  float64\n",
            " 5   sight_left        991346 non-null  float64\n",
            " 6   sight_right       991346 non-null  float64\n",
            " 7   hear_left         991346 non-null  float64\n",
            " 8   hear_right        991346 non-null  float64\n",
            " 9   SBP               991346 non-null  float64\n",
            " 10  DBP               991346 non-null  float64\n",
            " 11  BLDS              991346 non-null  float64\n",
            " 12  tot_chole         991346 non-null  float64\n",
            " 13  HDL_chole         991346 non-null  float64\n",
            " 14  LDL_chole         991346 non-null  float64\n",
            " 15  triglyceride      991346 non-null  float64\n",
            " 16  hemoglobin        991346 non-null  float64\n",
            " 17  urine_protein     991346 non-null  float64\n",
            " 18  serum_creatinine  991346 non-null  float64\n",
            " 19  SGOT_AST          991346 non-null  float64\n",
            " 20  SGOT_ALT          991346 non-null  float64\n",
            " 21  gamma_GTP         991346 non-null  float64\n",
            " 22  SMK_stat_type_cd  991346 non-null  float64\n",
            " 23  DRK_YN            991346 non-null  object \n",
            "dtypes: float64(19), int64(3), object(2)\n",
            "memory usage: 181.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are missing values in the dataset\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "duplicates = df.duplicated().sum()\n",
        "print(\"\")\n",
        "print(df.index[df.duplicated(keep=False)])\n",
        "print(\"duplicates : {:0.0f}\".format(duplicates))\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "duplicates = df.duplicated().sum()\n",
        "print(\"duplicates after clearing: {:0.0f}\".format(duplicates))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIBThWTcYRit",
        "outputId": "2fea9d9d-c54b-4df8-d64a-54261d4e7422"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sex                 0\n",
            "age                 0\n",
            "height              0\n",
            "weight              0\n",
            "waistline           0\n",
            "sight_left          0\n",
            "sight_right         0\n",
            "hear_left           0\n",
            "hear_right          0\n",
            "SBP                 0\n",
            "DBP                 0\n",
            "BLDS                0\n",
            "tot_chole           0\n",
            "HDL_chole           0\n",
            "LDL_chole           0\n",
            "triglyceride        0\n",
            "hemoglobin          0\n",
            "urine_protein       0\n",
            "serum_creatinine    0\n",
            "SGOT_AST            0\n",
            "SGOT_ALT            0\n",
            "gamma_GTP           0\n",
            "SMK_stat_type_cd    0\n",
            "DRK_YN              0\n",
            "dtype: int64\n",
            "\n",
            "Int64Index([ 12101,  36972,  39808,  61934,  82306,  82607,  99422, 115929,\n",
            "            118930, 126538, 133412, 159911, 175152, 184489, 186560, 211709,\n",
            "            231468, 246305, 271717, 280830, 284051, 284528, 290463, 323132,\n",
            "            335747, 354088, 429596, 445608, 453451, 471596, 479756, 555137,\n",
            "            558263, 568854, 606663, 626044, 629549, 668305, 671067, 686628,\n",
            "            727207, 746077, 770036, 779854, 794384, 803956, 804343, 834790,\n",
            "            872213, 953247, 973015, 982525],\n",
            "           dtype='int64')\n",
            "duplicates : 26\n",
            "duplicates after clearing: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which column needs fixing\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values in '{column}':\")\n",
        "    print(unique_values)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW1Wxt6zYThN",
        "outputId": "07da3acf-693b-4b81-a2cf-692a364f9bea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'sex':\n",
            "['Male' 'Female']\n",
            "\n",
            "Unique values in 'age':\n",
            "[35 30 40 50 45 55 65 25 60 20 70 75 80 85]\n",
            "\n",
            "Unique values in 'height':\n",
            "[170 180 165 175 150 155 160 145 140 185 135 190 130]\n",
            "\n",
            "Unique values in 'weight':\n",
            "[ 75  80  60  55  65  50  85  70  45  40  95 120  90  35 105 100 110 115\n",
            " 130  30 125 140  25 135]\n",
            "\n",
            "Unique values in 'waistline':\n",
            "[ 90.   89.   91.   80.   75.   69.   84.2  84.   82.   79.2  98.   72.3\n",
            "  88.   76.   73.   78.   99.   85.   67.   62.   92.   79.   87.   70.\n",
            "  67.5  87.3  71.   92.9  94.   79.3  77.   75.7  85.5  74.   60.   81.\n",
            "  72.   65.   63.   81.6  83.   61.  110.   86.8  73.5  93.  109.   54.\n",
            "  91.2  66.   79.5  86.   97.1  76.2  80.5  68.   64.   74.1  85.9  65.3\n",
            "  95.   94.5 100.   85.4  77.6  73.3 103.   93.5  67.8  69.2 105.7 105.\n",
            "  74.2  97.   75.4  83.2  88.5  85.3  87.4  71.5  64.1  76.6  93.1  84.8\n",
            "  88.1  66.8  96.   81.5  80.1  87.2  86.5 104.  114.   56.   88.8  89.2\n",
            "  66.2  90.8  88.2  82.5  65.4  72.2  81.3  75.6  87.8  77.2  98.5  85.2\n",
            "  97.5  63.5  95.4  72.6 110.8  81.2  87.5  89.5  82.4  81.8  76.5  87.1\n",
            "  78.1  72.7  84.3 101.6  84.5  98.7 101.   98.1  66.5  90.4  76.1  77.1\n",
            "  82.1  93.3  80.4  70.3  71.1 111.   90.1  70.6  65.1  83.8  81.4  68.1\n",
            " 106.   86.2  70.2 116.   83.3  69.8  58.5  84.7  74.5  71.4  86.7 104.2\n",
            " 112.   92.7 107.7  82.6  70.5  75.3 113.   59.   68.8  95.1  95.6 100.4\n",
            "  96.7  78.3  72.5  79.9 102.   92.2  91.8  73.2  79.8  90.6  95.7  77.8\n",
            "  60.5  70.4  96.2  82.2  81.1  86.3  80.8  94.2  66.4  63.2  63.7  68.5\n",
            " 101.8  93.8  96.1  97.2  79.6  91.3  83.5  65.5  99.9  78.8  58.  118.4\n",
            "  92.1  93.7  66.1  71.2  75.5 107.   68.2  87.7  78.4  94.1 115.   86.9\n",
            "  96.6  99.1 108.5  75.8  88.7  93.2  99.3  89.1  88.9  91.5  75.2  96.8\n",
            "  80.2  68.4  77.5  64.9  74.9  84.4  74.4  82.8  71.8  86.6  94.3  84.9\n",
            " 107.1  86.1  69.4  76.4  82.3  89.8 106.5  72.4 108.   87.6  80.7  90.3\n",
            "  73.4  78.2  92.6  74.6 105.5  65.9  78.5  94.6  95.2  64.5  91.4  62.5\n",
            "  83.6 108.2  69.5  90.2  93.4  81.7 122.4  86.4  88.3  99.2  85.6  75.9\n",
            "  83.1  61.5  98.4  64.8 118.   57.   76.9  68.6  63.6  82.9  92.5 128.\n",
            " 107.5  67.6 109.5  73.1  68.7  68.3 105.2  71.6  85.1  78.6  76.3  77.9\n",
            "  97.6  88.4  94.7  89.7  70.8 100.3  78.7  80.6  90.9  95.5  91.1  97.9\n",
            " 101.5  67.4  65.2  74.8  73.7  99.4  77.4  84.6  69.1  70.1  72.9  81.9\n",
            "  74.7 100.5  79.1  72.8  61.8  67.2  80.3  73.8  72.1 126.5 103.5  62.1\n",
            "  76.7  89.9  75.1 102.1  89.6  92.4 104.5  64.6  61.4  76.8  69.7 126.\n",
            "  62.7  92.3  59.2  96.5 103.3  84.1  74.3  59.7 104.8  97.4  80.9  73.9\n",
            " 109.2  83.7  89.4  90.5 103.2  65.8 101.1  89.3 105.6  69.3  51.   83.4\n",
            " 117.  111.4  94.4 105.4  98.9  92.8  82.7  99.5  70.9 106.2  79.4  73.6\n",
            "  79.7  83.9  99.7  61.6  88.6  55.   68.9  96.9  66.7  78.9 123.  103.1\n",
            "  94.8  91.9  95.8  95.3 102.6 103.6  77.3  85.8  62.8 111.8  62.2 105.1\n",
            "  62.4  87.9  63.9  93.9  99.8  57.1 101.3  66.6  67.9  49.   85.7 112.6\n",
            "  71.3  62.3  66.3  63.1  98.6  61.1  64.2  71.9  91.7  60.2  59.6  90.7\n",
            " 108.3  77.7 112.5 104.7  59.3  65.6 107.3  70.7  69.9  67.1  65.7  61.9\n",
            " 999.   61.2 102.5  67.3  63.8  91.6  97.3 109.7  98.3 100.1  57.7  94.9\n",
            " 112.2  98.8  56.7  55.4 106.9 113.4 100.7 119.  113.9  58.8  97.8  59.5\n",
            "  63.3  71.7 121.  108.9 108.6 116.4  64.7  63.4  64.3 110.9  95.9 100.2\n",
            "  60.1 102.2  67.7 109.6  96.3 108.7  98.2 102.3  56.1  64.4  93.6 122.\n",
            "  35.   58.2 120.  103.8 106.7 104.1 102.8 111.3  56.8 104.6 113.5 115.5\n",
            " 114.2  60.3  61.3  60.9  58.6 107.8 101.4 110.5  60.8 100.8 106.8  66.9\n",
            " 115.2 112.3 104.4 100.6 109.1  60.6  61.7 103.4  56.3  57.9  99.6  97.7\n",
            "  59.8 101.7 110.1 114.6  52.1  59.1  69.6 100.9 113.7  59.4  56.2 114.5\n",
            "  62.9  59.9  52.  110.4  96.4 107.4 123.8 109.3  57.8 107.6 119.5 102.4\n",
            " 111.5 110.7 117.2 120.7  53.  129.  103.7 104.3 101.2 102.7 104.9 112.8\n",
            " 122.5 114.7  62.6  58.7  55.6 109.8 105.8 106.1 117.7 108.1 113.8 109.4\n",
            " 118.5 105.3  55.5  54.4 101.9 125.   58.9  60.4  57.2 107.2 124.5 111.1\n",
            " 114.8  58.3 107.9  53.2  57.4 130.  145.   50.   55.8  60.7 106.3 106.6\n",
            " 110.3  54.3 111.2  56.9 113.1 136.  114.1 138.  102.9 124.  117.5  55.1\n",
            " 116.8 118.2 120.5 108.4  55.2  56.5 129.6 120.6 112.4 127.   58.4 109.9\n",
            " 116.5 116.3 124.2 121.4 132.  117.8 119.1 115.6 115.4 112.1  57.3 118.9\n",
            "  53.1  55.7  54.5  57.6 106.4  57.5 116.2 110.2 113.3 117.3  48.   27.\n",
            " 123.2 119.7 121.2  58.1 118.8 108.8 123.5 140.  103.9 112.7 120.1 119.2\n",
            "  52.6 105.9 113.2 118.1 115.8 120.2  53.4 111.7  56.4 130.5  55.9 110.6\n",
            "  42.   54.6 116.6 116.1 119.3  32.  114.3 121.3 117.9 134.   53.5  54.8\n",
            " 121.5  54.2  51.5  50.5 117.6  40.  115.3 114.9 117.1 126.6 119.8 115.7\n",
            "  52.5 115.9 119.4  43.   30.  133.  149.1 123.3 113.6  56.6  53.8  51.6\n",
            " 123.4  55.3  50.3 111.6 136.8  54.1 115.1 126.2  51.8 116.9 131.  135.\n",
            "  53.6 117.4  54.9  51.1 122.3   8.  121.1 116.7 125.5 111.9 127.3  52.4\n",
            " 122.6 124.1 123.1 118.3  51.2]\n",
            "\n",
            "Unique values in 'sight_left':\n",
            "[1.  0.9 1.2 1.5 0.5 0.7 0.3 0.6 0.8 0.2 9.9 0.4 0.1 2.  1.6 1.1 1.8 1.9\n",
            " 1.3 1.4 1.7 2.1 2.5 2.2]\n",
            "\n",
            "Unique values in 'sight_right':\n",
            "[1.  1.2 1.5 0.4 0.9 9.9 0.8 0.7 0.6 0.5 0.3 0.2 0.1 2.  1.6 1.4 1.8 1.1\n",
            " 1.9 1.7 1.3 2.1 2.5 2.2]\n",
            "\n",
            "Unique values in 'hear_left':\n",
            "[1. 2.]\n",
            "\n",
            "Unique values in 'hear_right':\n",
            "[1. 2.]\n",
            "\n",
            "Unique values in 'SBP':\n",
            "[120. 130. 145. 138. 142. 101. 132. 118. 109. 129. 113. 126. 119. 121.\n",
            " 111. 110. 122. 114. 115. 128. 133. 167. 127. 102. 137. 116. 108. 134.\n",
            " 131. 124. 100. 135. 136. 149. 146. 148. 123. 183. 140. 139.  90. 104.\n",
            " 112. 105. 106. 150. 184. 141.  94.  93. 125. 163. 103. 107.  99. 155.\n",
            " 151. 117. 180. 195. 144.  96.  98.  95. 143. 153. 158.  84. 156. 165.\n",
            " 152.  97. 172. 171. 160.  92. 147. 164.  91. 162. 154. 161. 157.  85.\n",
            " 168. 187. 170. 159.  82.  86. 188.  87.  89. 166.  88. 178. 175. 169.\n",
            " 194. 173. 190. 177. 230. 174.  75. 176. 229.  83. 203. 193.  80. 179.\n",
            "  79. 186. 181. 185. 204. 191. 182. 200. 207. 217. 192. 199. 206. 197.\n",
            " 202. 198. 210. 196.  77.  81. 212. 189. 208. 220.  78. 215. 232. 214.\n",
            " 219. 218. 201.  76. 241.  72. 236. 221. 224.  70. 211. 234. 222. 216.\n",
            " 270. 209.  74. 255. 205.  73. 240. 253. 244. 213. 235. 227. 223. 238.\n",
            " 228. 273.  67.]\n",
            "\n",
            "Unique values in 'DBP':\n",
            "[ 80.  82.  70.  87.  92.  58.  85. 105.  69.  72.  77.  78.  67.  74.\n",
            "  65.  76.  64.  55.  75.  83.  84. 102.  60.  62.  86.  90.  71.  94.\n",
            " 109.  73.  79.  66.  56.  63.  57.  81.  52.  68.  93.  89. 104.  95.\n",
            "  88. 100. 115.  53.  91.  61. 106.  97.  59.  54.  96.  51. 111. 112.\n",
            "  98. 110. 103.  99. 120. 134. 108. 114. 101. 123. 113.  47.  50. 130.\n",
            " 160. 119.  46. 107. 118. 126.  49.  45. 124. 117. 116. 135.  42. 140.\n",
            " 127. 125. 132.  48. 129. 128. 122.  43. 121. 133. 131.  44. 142.  41.\n",
            " 137.  36. 138. 150. 141. 145.  38. 163. 185.  37. 146. 143. 144. 139.\n",
            "  40.  39. 153. 154.  33. 156. 164. 181. 136.  34. 180. 149. 147. 170.\n",
            "  32.]\n",
            "\n",
            "Unique values in 'BLDS':\n",
            "[ 99. 106.  98.  95. 101.  89.  94. 104. 100.  90. 137.  82.  79.  96.\n",
            " 105.  88. 111. 102.  87.  83.  81.  91. 110.  93. 128. 103.  86.  80.\n",
            "  97.  85.  84. 112. 118. 155. 160. 114. 120. 107. 109. 140. 161. 119.\n",
            " 125.  92.  76. 324.  75.  77. 121.  73. 108.  78. 124. 192. 115.  74.\n",
            " 113. 217. 229.  68. 425.  70. 163.  69. 130.  71. 143. 212. 133. 129.\n",
            " 117. 144. 168. 162. 127. 193. 220. 138. 249. 200. 151. 169. 150. 116.\n",
            " 146. 148. 235. 136. 134.  62. 126. 135. 184. 123.  72. 290.  65. 153.\n",
            " 122. 142. 174. 131. 147. 154.  63. 164. 149. 139. 141. 306.  67. 181.\n",
            "  66. 187. 175. 152. 190. 238. 132. 159. 180.  61. 156. 277. 215. 209.\n",
            " 210. 145. 158. 205. 194. 266. 204. 239. 421. 244. 172. 214. 240. 195.\n",
            " 170. 257. 157. 271. 211. 346. 412. 268. 236. 207. 231. 177.  60. 196.\n",
            " 253. 247. 373. 191. 319. 304. 242. 213.  58. 241. 167. 185. 178. 293.\n",
            " 222. 173. 186. 246. 230. 298. 259. 243. 182. 300. 228. 166. 468. 171.\n",
            " 197. 302. 202. 176. 198. 337. 270. 419. 426. 219. 188.  64. 183. 296.\n",
            " 258. 370. 165. 232. 179. 227. 262. 255. 305. 223. 225. 322. 272. 252.\n",
            " 437. 339. 310. 201. 299. 208. 380. 250. 224. 189. 206. 233. 216. 199.\n",
            " 312. 343. 367. 218. 327. 264. 254. 269. 320. 203. 273. 280. 283. 308.\n",
            " 342. 234. 303. 261.  54. 251. 263.  55. 334. 278. 329. 307.  51. 260.\n",
            " 245. 221. 274. 333. 311. 287. 284. 297.  56. 341. 248. 286. 321. 309.\n",
            "  59. 275. 279. 288. 291. 281. 357. 364. 316. 336. 318.  52. 267. 276.\n",
            " 328. 372. 317. 292. 301. 285. 400. 331. 500. 447. 356. 536. 433. 289.\n",
            " 314. 596. 282.  50. 402. 256. 226. 349.  47. 313. 332. 456. 265. 578.\n",
            " 294. 386. 429. 295.  42.  48. 381.  57. 237. 368. 360. 353. 385. 411.\n",
            " 685. 344. 428. 355.  46. 407. 629. 394.  45. 326. 551. 315. 351. 383.\n",
            "  38. 363. 338. 398. 323. 335. 491. 352. 340. 616. 358. 330. 393. 586.\n",
            " 354. 365. 378. 452. 325. 374. 348. 424. 396. 415. 377. 382. 480.  49.\n",
            " 470. 663. 439. 345. 469. 390. 359. 403. 541. 569. 505. 560. 369. 467.\n",
            " 347. 409. 401. 538. 462. 350. 769. 463. 392.  44. 418. 375. 427. 579.\n",
            " 477. 362. 379. 423. 446.  32. 438. 366. 458. 577. 454. 434. 406. 376.\n",
            " 502. 404. 599. 564.  53. 598.  43. 450. 410. 460. 483. 497. 457. 361.\n",
            " 459. 499. 597. 405. 391. 617.  30. 397. 443. 444.  36. 482. 461. 430.\n",
            " 420. 431. 440. 558. 414. 552.  33. 517. 408. 387. 453.  39. 516. 371.\n",
            " 384. 478. 535. 416. 513. 395. 512. 511. 464. 627.  34. 800.  37. 455.\n",
            " 493. 472. 801. 413. 496. 484. 389. 487. 503. 442. 476. 435. 561. 445.\n",
            " 436. 471. 522. 611. 588. 638. 486. 481. 784.  40. 449. 595. 388. 546.\n",
            " 852.  25. 741. 563. 495. 498. 527. 548.]\n",
            "\n",
            "Unique values in 'tot_chole':\n",
            "[ 193.  228.  136.  201.  199.  218.  196.  185.  217.  195.  183.  115.\n",
            "  200.  205.  113.  148.  147.  180.  197.  174.  207.  170.  156.  154.\n",
            "  293.  231.  246.  191.  212.  162.  208.  210.  232.  142.  181.  240.\n",
            "  209.  137.  241.  153.  219.  235.  263.  215.  224.  245.  167.  203.\n",
            "  155.  175.  315.  211.  250.  273.  277.  169.  233.  134.  135.  242.\n",
            "  150.  198.  223.  225.  244.  213.  165.  116.  172.  149.  262.  110.\n",
            "  214.  247.  179.  187.  152.  192.  168.  220.  272.  264.  138.  140.\n",
            "  161.  157.  186.  189.  230.  226.  158.  132.   95.  143.  299.  243.\n",
            "  280.  257.  236.  190.  222.  206.  278.  194.  202.  287.  144.  204.\n",
            "  159.  121.  163.  164.  166.  234.  151.  221.  160.  258.  129.  274.\n",
            "  259.  239.   90.  177.  141.  188.  184.  178.  384.  182.  290.  291.\n",
            "  265.  216.  268.  251.  119.  227.  139.  104.  100.  252.  173.  229.\n",
            "  124.  248.  130.  283.  102.  146.  171.  254.  238.  237.  275.  276.\n",
            "  285.  117.  270.  253.  266.  269.  145.  176.  284.  126.  131.  122.\n",
            "  103.  256.  282.  111.  133.  123.  255.  249.  267.  292.  307.  279.\n",
            "  320.  281.  306.  127.  288.  294.  261.  260.  303.  109.  289.  335.\n",
            "  271.  310.  304.  337.  308.  301.  125.  323.  311.  305.   98.  118.\n",
            "  105.  112.  369.  120.  286.  331.  128.  106.  114.  302.  309.  367.\n",
            "  348.  317.  295.   77.  312.  107.  316.  108.  298.  352.  313.  101.\n",
            "  349.  296.   73.  334.  322.  357.  326.  297. 1619.   91.  400.  600.\n",
            "   87.  330.   83.  343.  319.   94.   96.  338.   97.  324.  360.  346.\n",
            "  377.  300.  336.  314.  339.   86.  342.   57.  318.   54.  350.   84.\n",
            "  362.  321.  386.  325.  333.  395.  372.  358.   75.  426.  373.  329.\n",
            "  355.   92.  347.  344.  356.  327.  353.   82.  370.   88.  404.  405.\n",
            "   99.   85.  340.  328.  383.  413.  332.  700.  351.  398.  354.  500.\n",
            "  428.  345.   89.  361.  363.   81.  387.  388.  492.   93.  406.  412.\n",
            "  341.  423.  464.   78.  366.  567.  633.   76.  420.  452. 1030.  374.\n",
            "  368.  418.  650.  433.  380.  393.  474.  359.  468.   80.  365.  376.\n",
            "  407.  397.  394.  559.  408.   72.   69.  403.  415.  390.  424.  382.\n",
            "  656.  599.  483.  385.  744.  715. 1196.  409.   62.  391.  389.  441.\n",
            "  642.  435.  414.  482.  364.  506.  479.   68.  447.  442.   74.   60.\n",
            "  416.  450.  429.  730.  478.  379.  460.  417.  440.  401.  378.  725.\n",
            "  511.  475.  410.  439.  411.  371. 2196.  524.  514.  392.  532.   63.\n",
            "  396.  381.   71.  462.   64.  472.  453.   70.  431.  497.   79.  375.\n",
            "  607.  502. 1575. 2046.  399.  430. 2067.  485.  422. 1815. 1605.  563.\n",
            "  508.  489.  753.  557.  470.  710.  454.  438. 1736.  451.   58.  436.\n",
            "  432.  458.  522.  556.   30.  535. 1446.  427.  604.  523.  515.  480.\n",
            "  621.  837.  484.  421.   45.  445.  564.   55.   59.  662.  623.  419.\n",
            "   65. 2344.  446.  491.  465.  456.  897.  469.  473. 1306. 1315.  574.\n",
            " 2033.  792. 1335. 1191.   67.  467.]\n",
            "\n",
            "Unique values in 'HDL_chole':\n",
            "[4.800e+01 5.500e+01 4.100e+01 7.600e+01 6.100e+01 7.700e+01 6.600e+01\n",
            " 5.800e+01 5.600e+01 6.000e+01 4.200e+01 3.100e+01 5.100e+01 5.300e+01\n",
            " 4.400e+01 5.400e+01 4.300e+01 6.200e+01 3.900e+01 5.700e+01 5.900e+01\n",
            " 3.500e+01 2.900e+01 8.500e+01 5.000e+01 4.700e+01 1.000e+02 7.000e+01\n",
            " 4.600e+01 6.300e+01 4.500e+01 8.700e+01 6.400e+01 7.200e+01 6.700e+01\n",
            " 7.100e+01 6.900e+01 6.500e+01 4.000e+01 3.200e+01 4.900e+01 3.800e+01\n",
            " 8.400e+01 7.300e+01 9.000e+01 3.700e+01 2.800e+01 6.800e+01 3.400e+01\n",
            " 8.300e+01 7.400e+01 5.200e+01 8.600e+01 8.000e+01 7.900e+01 1.190e+02\n",
            " 7.500e+01 1.180e+02 3.300e+01 9.900e+01 9.300e+01 8.800e+01 9.100e+01\n",
            " 9.200e+01 8.200e+01 3.600e+01 7.800e+01 8.100e+01 1.030e+02 9.600e+01\n",
            " 9.400e+01 1.050e+02 9.800e+01 2.200e+01 1.070e+02 3.000e+01 1.120e+02\n",
            " 9.700e+01 9.500e+01 8.900e+01 1.110e+02 1.080e+02 1.200e+02 1.040e+02\n",
            " 2.600e+01 2.700e+01 1.130e+02 2.300e+01 1.060e+02 1.010e+02 1.100e+02\n",
            " 2.500e+01 1.020e+02 1.090e+02 4.000e+00 2.100e+01 1.640e+02 2.400e+01\n",
            " 1.570e+02 1.230e+02 1.150e+02 8.110e+03 1.240e+02 4.140e+02 1.900e+01\n",
            " 6.000e+00 1.340e+02 1.450e+02 1.260e+02 1.370e+02 1.270e+02 1.400e+02\n",
            " 1.160e+02 1.100e+01 2.000e+01 5.260e+02 1.170e+02 1.140e+02 1.310e+02\n",
            " 1.800e+01 1.220e+02 1.250e+02 1.350e+02 1.280e+02 1.300e+02 7.000e+00\n",
            " 1.700e+01 1.290e+02 3.000e+00 1.550e+02 1.210e+02 1.300e+01 1.400e+01\n",
            " 1.410e+02 1.430e+02 1.530e+02 1.360e+02 1.380e+02 2.000e+00 1.390e+02\n",
            " 1.500e+01 1.890e+02 5.660e+02 2.270e+02 1.830e+02 4.270e+02 1.480e+02\n",
            " 2.010e+02 1.600e+01 1.320e+02 3.050e+02 1.580e+02 1.510e+02 1.700e+02\n",
            " 1.200e+01 1.610e+02 1.420e+02 5.200e+02 1.460e+02 1.000e+01 1.470e+02\n",
            " 1.500e+02 4.760e+02 1.660e+02 5.350e+02 4.840e+02 1.520e+02 1.590e+02\n",
            " 9.000e+00 1.490e+02 1.000e+00 1.330e+02 1.440e+02 4.770e+02 1.770e+02\n",
            " 1.710e+02 4.090e+02 1.960e+02 1.630e+02 7.970e+02 3.530e+02 4.750e+02\n",
            " 6.360e+02 1.206e+03 5.000e+00 1.600e+02 7.010e+02 5.230e+02 1.800e+02\n",
            " 1.540e+02 1.900e+02 5.420e+02 1.650e+02 6.970e+02 8.000e+00 6.770e+02\n",
            " 5.920e+02 5.650e+02 1.760e+02 1.780e+02 2.090e+02 4.150e+02 1.920e+02\n",
            " 6.580e+02 1.560e+02 4.300e+02 2.220e+02 1.620e+02 4.470e+02 3.540e+02\n",
            " 5.250e+02 1.680e+02 9.330e+02 5.080e+02 3.670e+02 6.180e+02 5.700e+02\n",
            " 4.410e+02 7.270e+02 1.810e+02 1.820e+02 1.870e+02 2.120e+02]\n",
            "\n",
            "Unique values in 'LDL_chole':\n",
            "[1.260e+02 1.480e+02 7.400e+01 1.040e+02 1.170e+02 9.500e+01 1.150e+02\n",
            " 1.070e+02 1.410e+02 1.180e+02 1.300e+02 5.700e+01 8.900e+01 1.290e+02\n",
            " 1.080e+02 6.200e+01 8.200e+01 8.500e+01 1.030e+02 1.110e+02 9.800e+01\n",
            " 1.340e+02 6.400e+01 1.130e+02 8.600e+01 2.150e+02 1.520e+02 1.000e+02\n",
            " 1.500e+02 9.400e+01 1.510e+02 1.020e+02 1.100e+02 7.200e+01 1.460e+02\n",
            " 1.280e+02 1.010e+02 7.700e+01 1.190e+02 1.630e+02 1.780e+02 1.430e+02\n",
            " 1.270e+02 1.530e+02 1.380e+02 1.050e+02 2.110e+02 1.360e+02 1.060e+02\n",
            " 1.650e+02 1.710e+02 1.950e+02 1.560e+02 1.420e+02 7.100e+01 9.200e+01\n",
            " 6.800e+01 1.310e+02 1.640e+02 6.100e+01 8.800e+01 1.400e+02 1.330e+02\n",
            " 1.240e+02 1.470e+02 5.100e+01 1.450e+02 7.900e+01 5.300e+01 9.900e+01\n",
            " 8.000e+01 1.140e+02 9.100e+01 1.700e+02 1.540e+02 4.800e+01 9.600e+01\n",
            " 1.550e+02 1.320e+02 7.800e+01 9.000e+01 1.220e+02 1.890e+02 1.830e+02\n",
            " 5.400e+01 8.300e+01 6.500e+01 6.700e+01 1.210e+02 2.600e+01 1.660e+02\n",
            " 8.400e+01 1.670e+02 9.700e+01 1.990e+02 1.390e+02 5.000e+01 1.810e+02\n",
            " 1.160e+02 1.250e+02 1.730e+02 1.590e+02 1.740e+02 2.020e+02 1.370e+02\n",
            " 1.570e+02 2.210e+02 1.090e+02 4.100e+01 7.600e+01 3.600e+01 1.350e+02\n",
            " 1.120e+02 8.100e+01 1.840e+02 5.200e+01 1.620e+02 1.580e+02 2.900e+01\n",
            " 4.500e+01 1.200e+02 7.000e+01 5.800e+01 1.490e+02 3.250e+02 4.700e+01\n",
            " 1.440e+02 1.980e+02 1.900e+02 5.500e+01 9.300e+01 7.300e+01 1.230e+02\n",
            " 4.000e+01 4.900e+01 3.300e+01 6.900e+01 6.600e+01 5.900e+01 1.760e+02\n",
            " 2.500e+01 8.700e+01 7.500e+01 1.690e+02 6.300e+01 1.820e+02 1.600e+02\n",
            " 4.400e+01 4.300e+01 1.790e+02 2.240e+02 6.000e+01 5.600e+01 3.800e+01\n",
            " 1.940e+02 1.750e+02 1.770e+02 1.720e+02 4.200e+01 1.680e+02 1.860e+02\n",
            " 1.800e+02 1.880e+02 1.930e+02 2.170e+02 2.060e+02 1.960e+02 2.000e+02\n",
            " 3.000e+01 1.910e+02 2.040e+02 2.090e+02 1.850e+02 1.610e+02 1.400e+01\n",
            " 3.900e+01 1.700e+01 1.920e+02 3.500e+01 2.080e+02 2.160e+02 2.260e+02\n",
            " 2.700e+01 4.600e+01 2.650e+02 2.070e+02 2.360e+02 1.870e+02 2.190e+02\n",
            " 2.180e+02 2.130e+02 2.440e+02 3.700e+01 2.230e+02 2.200e+02 2.710e+02\n",
            " 2.410e+02 2.100e+02 2.100e+01 1.970e+02 3.200e+01 8.000e+00 2.010e+02\n",
            " 3.400e+01 2.050e+02 1.000e+01 2.030e+02 2.280e+02 2.200e+01 3.100e+01\n",
            " 2.120e+02 1.300e+01 2.000e+01 2.490e+02 2.340e+02 1.800e+01 2.330e+02\n",
            " 2.430e+02 2.290e+02 2.250e+02 5.119e+03 1.200e+01 2.140e+02 2.570e+02\n",
            " 1.600e+01 2.400e+01 2.450e+02 2.300e+01 2.370e+02 2.610e+02 2.420e+02\n",
            " 2.220e+02 6.000e+00 1.000e+00 2.760e+02 2.300e+02 2.460e+02 2.470e+02\n",
            " 2.520e+02 2.390e+02 2.810e+02 2.320e+02 2.350e+02 2.800e+01 7.000e+00\n",
            " 2.620e+02 2.780e+02 2.270e+02 1.100e+01 2.310e+02 1.900e+01 2.600e+02\n",
            " 2.400e+02 9.000e+00 2.730e+02 2.840e+02 2.820e+02 2.000e+00 2.480e+02\n",
            " 2.940e+02 2.910e+02 2.380e+02 3.220e+02 2.510e+02 2.590e+02 2.950e+02\n",
            " 3.210e+02 1.500e+01 2.530e+02 2.630e+02 2.550e+02 2.640e+02 3.100e+02\n",
            " 3.500e+02 2.880e+02 3.190e+02 2.900e+02 5.000e+00 3.090e+02 2.770e+02\n",
            " 2.700e+02 3.230e+02 2.850e+02 2.540e+02 3.390e+02 2.690e+02 2.500e+02\n",
            " 4.000e+00 4.040e+02 3.310e+02 1.660e+03 2.660e+02 2.790e+02 2.890e+02\n",
            " 1.750e+03 2.970e+02 3.030e+02 3.000e+00 2.960e+02 3.050e+02 2.680e+02\n",
            " 3.530e+02 3.930e+02 1.410e+03 6.510e+02 2.860e+02 2.740e+02 4.670e+02\n",
            " 2.580e+02 3.020e+02 3.240e+02 1.484e+03 2.920e+02 3.000e+02 3.380e+02\n",
            " 7.020e+02 3.040e+02 5.810e+02 1.126e+03 3.180e+02 2.750e+02 3.900e+02\n",
            " 5.000e+02 4.320e+02 6.710e+02 8.800e+02 2.670e+02 1.182e+03 3.120e+02\n",
            " 3.860e+02 5.630e+02 3.160e+02 2.800e+02 3.070e+02 3.420e+02 2.720e+02\n",
            " 3.590e+02 2.560e+02 1.044e+03 2.830e+02 2.870e+02 2.980e+02 3.440e+02\n",
            " 2.990e+02 3.510e+02 3.110e+02 3.130e+02 3.280e+02 3.970e+02 2.114e+03\n",
            " 4.010e+02 3.600e+02 3.200e+02 6.430e+02 3.080e+02 3.690e+02 3.170e+02\n",
            " 3.610e+02 3.840e+02 7.590e+02 4.160e+02 1.541e+03 2.026e+03 3.140e+02\n",
            " 8.540e+02 2.043e+03 3.770e+02 4.130e+02 1.798e+03 1.580e+03 3.430e+02\n",
            " 1.128e+03 4.710e+02 6.870e+02 4.820e+02 3.740e+02 3.880e+02 1.696e+03\n",
            " 3.300e+02 3.460e+02 5.490e+02 4.200e+02 3.360e+02 3.830e+02 3.800e+02\n",
            " 8.100e+02 4.440e+02 3.330e+02 2.930e+02 3.270e+02 1.425e+03 3.410e+02\n",
            " 3.150e+02 6.890e+02 3.010e+02 8.160e+02 1.461e+03 4.360e+02 7.400e+02\n",
            " 7.320e+02 3.940e+02 9.630e+02 1.200e+03 3.480e+02 3.550e+02 3.540e+02\n",
            " 3.350e+02 7.700e+02 2.254e+03 2.111e+03 5.370e+02 3.750e+02 7.800e+02\n",
            " 4.960e+02 3.260e+02 3.620e+02 6.200e+02 4.510e+02 3.630e+02 4.430e+02\n",
            " 4.310e+02 4.110e+02 1.293e+03 3.290e+02 1.298e+03 3.340e+02 3.680e+02\n",
            " 1.933e+03 1.311e+03 3.650e+02 1.476e+03 9.850e+02]\n",
            "\n",
            "Unique values in 'triglyceride':\n",
            "[  92.  121.  104. ... 1448. 1932. 1638.]\n",
            "\n",
            "Unique values in 'hemoglobin':\n",
            "[17.1 15.8 17.6 13.8 12.3 14.4 15.1 13.9 12.9 16.5 13.1 15.7 14.5 16.\n",
            " 14.8 15.2 12.1 16.7 12.5 16.1 15.9 12.8 14.3 15.  14.6 15.3 11.4 15.4\n",
            " 14.  10.8 15.6 11.8 13.2 13.4 12.6 13.6 13.5 17.2 16.4 12.4 12.2 14.1\n",
            " 17.  14.9 14.2 10.3 13.7 18.1 11.2 11.7 14.7 13.3 16.3 17.8 12.7 12.\n",
            " 11.5 10.5 15.5 10.1 10.2 16.2 13.  16.9 10.7 17.9  9.8 16.6 17.4 11.6\n",
            " 11.1 17.7 11.9 11.   9.3 16.8 10.6 11.3 10.   9.6 18.   9.5 17.3 18.4\n",
            "  7.2 17.5  9.2  8.8  9.7 10.9  8.3 19.8 10.4  9.   7.5 19.3  7.4 18.8\n",
            "  8.9  8.7  9.1  9.9  9.4  6.5 18.7 18.6  7.9 18.2  8.4  6.2 18.3 18.5\n",
            "  8.2 20.3  8.6  8.   8.1  8.5  7.7  7.1  7.8  7.6  6.1  6.6  7.3  6.7\n",
            " 20.2 19.4 18.9  7.  19.1  6.9  6.4  6.8 19.   6.3  1.   5.5 19.2  4.7\n",
            " 20.9 19.5 21.  19.9  5.9 23.9 19.7 20.8 19.6  4.4  5.7  4.3  5.8  4.1\n",
            "  4.9  5.4  3.9  5.3 24.2  5.1  6.  20.1  5.6  5.  20.7  3.7 21.3  5.2\n",
            "  4.2 20.5  4.  20.  22.7 22.  25.  21.7  4.8 21.2 21.6 22.1 20.6  3.8\n",
            "  4.5 23.6  2.8 23.3 21.5  4.6 21.8 21.1]\n",
            "\n",
            "Unique values in 'urine_protein':\n",
            "[1. 3. 2. 4. 5. 6.]\n",
            "\n",
            "Unique values in 'serum_creatinine':\n",
            "[ 1.   0.9  1.1  0.8  1.3  0.6  0.5  1.2  0.7  0.4  0.2  1.4  1.7  5.\n",
            "  3.   0.3  4.6  1.5  1.9  1.6  9.5  5.3  1.8 10.   2.4  8.   3.3  2.9\n",
            "  2.   0.1  2.1  4.5  6.4  2.2 16.4  2.7  7.   2.3  4.2  7.8 11.5  5.9\n",
            "  7.9  7.3 11.3  5.4 23.   2.6 15.7 11.6  2.5  8.1 13.9  6.   3.9 10.9\n",
            "  8.7 12.7  6.9  7.4  3.1 14.2 13.4 10.8  6.5  4.4 32.   6.1  4.8 12.\n",
            "  3.7  3.4 81.  37.   5.6  3.8  2.8 12.5  9.4  6.2 14.3  5.7  3.6  3.2\n",
            "  4.9  5.1  4.3  5.5  5.2  8.4  9.6  9.3  3.5  6.8  9.8  6.3 21.  94.\n",
            " 11.7  8.2 10.5 12.8  9.1 13.  76.  10.3  4.7 10.4  4.1  8.5 14.1  8.6\n",
            " 10.2 12.3  7.6  9.2 85.   7.7 14.   9.  11.9  8.9  7.5 18.5  6.6  7.2\n",
            "  4.   8.8 11.4 95.   8.3  5.8 15.9 13.1  9.9 14.4 12.9 11.2 13.7 16.6\n",
            " 18.1  7.1  9.7 11.  12.4  6.7 80.  10.7 11.8 13.8 24.4 19.  87.  93.\n",
            " 16.7 14.5 96.  98.  18.8 14.7 78.  16.8 12.6 75.  35.  20.  11.1 15.4\n",
            " 10.6 68.  26.  22.  13.3 12.1 13.2 79.  19.6 41.  15.5 39.  29.  12.2\n",
            " 10.1]\n",
            "\n",
            "Unique values in 'SGOT_AST':\n",
            "[2.100e+01 2.000e+01 4.700e+01 2.900e+01 1.900e+01 1.800e+01 3.200e+01\n",
            " 4.800e+01 1.300e+01 1.500e+01 1.200e+01 3.300e+01 4.100e+01 2.800e+01\n",
            " 1.700e+01 1.600e+01 3.100e+01 3.900e+01 2.500e+01 4.300e+01 2.200e+01\n",
            " 3.800e+01 3.000e+01 1.400e+01 2.300e+01 3.400e+01 4.600e+01 2.400e+01\n",
            " 2.700e+01 1.100e+01 6.700e+01 2.600e+01 4.000e+01 3.500e+01 4.900e+01\n",
            " 5.900e+01 3.600e+01 4.200e+01 4.400e+01 5.200e+01 1.410e+02 3.700e+01\n",
            " 6.600e+01 5.000e+01 5.500e+01 4.500e+01 4.000e+00 9.100e+01 5.100e+01\n",
            " 8.300e+01 5.600e+01 6.800e+01 7.500e+01 2.780e+02 1.000e+01 5.400e+01\n",
            " 6.900e+01 1.060e+02 1.000e+02 1.570e+02 6.000e+01 6.200e+01 7.400e+01\n",
            " 5.700e+01 5.000e+00 2.200e+02 7.900e+01 1.580e+02 9.000e+01 5.300e+01\n",
            " 7.800e+01 1.030e+02 1.330e+02 8.800e+01 6.500e+01 8.000e+01 8.600e+01\n",
            " 5.800e+01 1.510e+02 1.530e+02 9.300e+01 7.200e+01 9.400e+01 7.700e+01\n",
            " 1.670e+02 7.300e+01 6.100e+01 1.230e+02 8.400e+01 9.200e+01 8.200e+01\n",
            " 2.240e+02 7.000e+01 1.450e+02 7.600e+01 6.300e+01 6.400e+01 6.000e+00\n",
            " 1.270e+02 9.500e+01 9.700e+01 9.000e+00 8.000e+00 9.110e+02 8.700e+01\n",
            " 1.900e+02 7.100e+01 1.930e+02 1.440e+02 1.420e+02 1.470e+02 1.050e+02\n",
            " 1.070e+02 1.110e+02 8.500e+01 1.220e+02 1.560e+02 1.380e+02 8.100e+01\n",
            " 1.240e+02 1.200e+02 9.600e+01 1.320e+02 1.090e+02 1.020e+02 2.370e+02\n",
            " 2.770e+02 2.180e+02 1.490e+02 1.080e+02 1.150e+02 1.820e+02 2.640e+02\n",
            " 1.160e+02 2.920e+02 1.180e+02 1.210e+02 3.070e+02 1.300e+02 1.140e+02\n",
            " 1.290e+02 1.010e+02 1.250e+02 7.000e+00 8.900e+01 1.460e+02 2.510e+02\n",
            " 1.610e+02 1.880e+02 2.080e+02 1.810e+02 9.800e+01 1.130e+02 1.350e+02\n",
            " 1.680e+02 2.130e+02 9.900e+01 1.360e+02 1.120e+02 1.800e+02 1.040e+02\n",
            " 1.310e+02 1.190e+02 2.950e+02 1.690e+02 1.260e+02 2.170e+02 1.390e+02\n",
            " 3.230e+02 3.560e+02 9.240e+02 2.060e+02 2.620e+02 1.990e+02 2.010e+02\n",
            " 1.860e+02 2.740e+02 2.400e+02 3.000e+00 1.100e+02 4.540e+02 1.540e+02\n",
            " 1.280e+02 1.500e+02 1.660e+02 1.650e+02 3.710e+02 1.740e+02 1.730e+02\n",
            " 7.250e+02 1.590e+02 2.000e+02 2.460e+02 2.210e+02 2.150e+02 1.170e+02\n",
            " 1.400e+02 1.340e+02 8.360e+02 3.810e+02 7.120e+02 2.990e+02 1.911e+03\n",
            " 1.760e+02 2.880e+02 1.790e+02 2.050e+02 1.840e+02 1.870e+03 3.970e+02\n",
            " 1.480e+02 1.600e+02 1.750e+02 1.520e+02 3.910e+02 1.000e+00 6.230e+02\n",
            " 2.090e+02 2.100e+02 7.970e+02 1.780e+02 3.140e+02 1.550e+02 1.770e+02\n",
            " 3.460e+02 2.680e+02 5.050e+02 3.370e+02 2.410e+02 1.960e+02 1.430e+02\n",
            " 5.540e+02 1.950e+02 1.370e+02 3.250e+02 2.350e+02 3.530e+02 3.310e+02\n",
            " 3.210e+02 1.640e+02 2.030e+02 2.500e+02 1.890e+02 2.480e+02 2.290e+02\n",
            " 4.620e+02 6.150e+02 2.120e+02 2.250e+02 3.990e+02 3.700e+02 3.260e+02\n",
            " 4.140e+02 2.690e+02 7.540e+02 2.540e+02 3.180e+02 3.610e+02 2.850e+02\n",
            " 3.040e+02 1.066e+03 2.160e+02 1.630e+02 2.000e+00 3.020e+02 4.480e+02\n",
            " 2.390e+02 2.790e+02 4.770e+02 2.820e+02 1.720e+02 2.610e+02 4.550e+02\n",
            " 5.740e+02 1.700e+02 2.280e+02 2.260e+02 4.260e+02 2.300e+02 2.890e+02\n",
            " 3.100e+02 3.930e+02 1.830e+02 3.010e+02 2.330e+02 3.600e+02 6.400e+02\n",
            " 2.360e+02 5.240e+02 4.300e+02 3.220e+02 1.620e+02 3.380e+02 4.210e+02\n",
            " 2.470e+02 1.870e+02 8.030e+02 3.820e+02 2.870e+02 1.850e+02 3.760e+02\n",
            " 1.920e+02 4.700e+02 1.910e+02 3.350e+02 7.600e+02 5.160e+02 3.960e+02\n",
            " 3.410e+02 5.800e+02 1.940e+02 1.970e+02 3.920e+02 2.720e+02 4.710e+02\n",
            " 2.230e+02 2.970e+02 5.310e+02 5.360e+02 5.220e+02 2.430e+02 3.400e+02\n",
            " 3.160e+02 4.490e+02 8.900e+02 3.330e+02 1.980e+02 6.030e+02 2.750e+02\n",
            " 2.020e+02 2.040e+02 4.270e+02 2.910e+02 3.800e+02 2.900e+02 2.190e+02\n",
            " 5.090e+02 6.270e+02 2.660e+02 2.420e+02 2.450e+02 4.910e+02 2.220e+02\n",
            " 4.070e+02 4.760e+02 8.760e+02 1.029e+03 4.170e+02 2.600e+02 5.650e+02\n",
            " 3.870e+02 2.980e+02 4.100e+02 6.000e+02 1.710e+02 9.990e+02 1.600e+03\n",
            " 4.220e+02 2.310e+02 3.830e+02 4.400e+02 2.810e+02 4.630e+02 3.280e+02\n",
            " 2.840e+02 3.440e+02 2.320e+02 3.110e+02 8.340e+02 5.000e+02 2.630e+02\n",
            " 4.450e+02 4.020e+02 2.440e+02 2.110e+02 2.930e+02 8.120e+02 1.686e+03\n",
            " 3.000e+02 1.436e+03 2.530e+02 3.650e+02 2.550e+02 9.440e+02 3.640e+02\n",
            " 4.230e+02 7.940e+02 3.390e+02 3.090e+02 3.720e+02 3.240e+02 7.000e+02\n",
            " 1.650e+03 6.780e+02 2.340e+02 6.090e+02 1.164e+03 3.060e+02 6.260e+02\n",
            " 2.710e+02 3.540e+02 6.120e+02 2.590e+02 5.620e+02 3.620e+02 3.740e+02\n",
            " 4.420e+02 6.870e+02 2.860e+02 2.070e+02 2.140e+02 4.320e+02 4.060e+02\n",
            " 3.520e+02 4.000e+02 2.940e+02 2.580e+02 4.740e+02 6.180e+02 2.670e+02\n",
            " 3.690e+02 4.240e+02 2.800e+02 5.640e+02 3.860e+02 3.300e+02 3.940e+02\n",
            " 4.160e+02 5.110e+02 3.470e+02 3.840e+02 3.580e+02 5.750e+02 2.960e+02\n",
            " 4.990e+02 3.980e+02 3.570e+02 2.560e+02 5.080e+02 4.870e+02 2.830e+02\n",
            " 7.780e+02 6.690e+02 5.680e+02 3.590e+02 7.290e+02 2.570e+02 3.510e+02\n",
            " 5.070e+02 3.290e+02 1.540e+03 4.860e+02 3.742e+03 3.030e+02 6.940e+02\n",
            " 3.680e+02 3.440e+03 3.080e+02 6.380e+02 9.999e+03 4.940e+02 4.340e+02\n",
            " 1.026e+03 3.120e+02 4.090e+02 3.050e+02 4.050e+02 1.454e+03 5.180e+02\n",
            " 4.560e+02 4.190e+02 4.670e+02 4.460e+02 2.270e+02 2.730e+02 6.550e+02\n",
            " 9.230e+02 1.240e+03 5.410e+02 5.130e+02 1.052e+03 2.380e+02 4.690e+02\n",
            " 3.450e+02 5.260e+02 4.370e+02 4.500e+02 5.550e+02 3.480e+02 2.490e+02\n",
            " 2.520e+02 4.750e+02 3.420e+02 7.340e+02 7.000e+03 5.590e+02 5.630e+02\n",
            " 4.180e+02 8.720e+02 3.430e+02 4.440e+02 3.235e+03 3.130e+02 2.650e+02\n",
            " 2.700e+02 6.640e+02 9.150e+02 1.377e+03 8.470e+02 3.270e+02 3.490e+02\n",
            " 8.600e+02 9.700e+02 3.660e+02 7.800e+02 3.200e+02 5.610e+02 7.500e+02\n",
            " 1.041e+03 8.670e+02 7.460e+02 6.220e+02 7.580e+02 7.720e+02 7.020e+02\n",
            " 4.590e+02 6.170e+02 4.110e+02 6.080e+02 9.790e+02 6.290e+02 5.210e+02\n",
            " 1.318e+03 6.420e+02 7.810e+02 2.760e+02 7.440e+02 1.509e+03 1.078e+03\n",
            " 8.930e+02 3.900e+02 5.790e+02 3.670e+02 3.770e+02 2.670e+03 5.140e+02\n",
            " 7.090e+02 4.970e+02 9.580e+02 3.150e+02 8.270e+02 6.430e+02 4.900e+02\n",
            " 3.500e+02 1.962e+03 3.190e+02 4.800e+02 5.190e+02 4.640e+02 6.240e+02\n",
            " 4.830e+02 5.500e+02 8.200e+02 6.360e+02 6.650e+02 9.880e+02 7.530e+02\n",
            " 3.360e+02]\n",
            "\n",
            "Unique values in 'SGOT_ALT':\n",
            "[3.500e+01 3.600e+01 3.200e+01 3.400e+01 1.200e+01 4.000e+01 1.800e+01\n",
            " 2.300e+01 3.800e+01 1.400e+01 5.100e+01 2.000e+01 1.600e+01 2.400e+01\n",
            " 1.000e+01 7.000e+00 1.700e+01 1.300e+01 3.000e+01 2.100e+01 3.100e+01\n",
            " 1.100e+01 2.900e+01 4.300e+01 1.500e+01 4.500e+01 4.600e+01 2.500e+01\n",
            " 1.900e+01 2.700e+01 4.200e+01 8.000e+00 5.200e+01 1.110e+02 3.700e+01\n",
            " 9.000e+00 6.000e+00 6.200e+01 5.600e+01 4.800e+01 3.300e+01 7.700e+01\n",
            " 2.600e+01 2.200e+01 5.000e+01 6.700e+01 2.800e+01 3.900e+01 5.500e+01\n",
            " 5.400e+01 6.400e+01 4.700e+01 6.000e+01 4.100e+01 7.500e+01 6.100e+01\n",
            " 9.000e+01 5.800e+01 1.060e+02 4.400e+01 9.600e+01 7.800e+01 1.670e+02\n",
            " 6.500e+01 6.300e+01 1.100e+02 4.900e+01 6.600e+01 9.200e+01 3.110e+02\n",
            " 5.300e+01 7.600e+01 5.900e+01 8.900e+01 2.360e+02 1.220e+02 8.800e+01\n",
            " 1.210e+02 1.450e+02 8.000e+01 5.700e+01 6.800e+01 9.700e+01 8.500e+01\n",
            " 5.000e+00 7.400e+01 8.700e+01 1.380e+02 6.900e+01 1.030e+02 1.930e+02\n",
            " 1.150e+02 1.240e+02 8.300e+01 2.270e+02 8.200e+01 7.000e+01 9.800e+01\n",
            " 8.100e+01 1.190e+02 7.300e+01 3.210e+02 1.010e+02 7.900e+01 1.070e+02\n",
            " 1.180e+02 7.100e+01 2.320e+02 9.500e+01 7.200e+01 1.120e+02 1.160e+02\n",
            " 1.610e+02 2.020e+02 1.040e+02 9.100e+01 1.000e+02 1.280e+02 2.070e+02\n",
            " 1.860e+02 1.520e+02 8.400e+01 1.090e+02 1.270e+02 8.600e+01 9.900e+01\n",
            " 1.260e+02 9.400e+01 4.000e+00 9.300e+01 1.310e+02 1.330e+02 1.050e+02\n",
            " 1.570e+02 4.300e+02 1.790e+02 1.870e+02 1.130e+02 1.530e+02 1.350e+02\n",
            " 1.020e+02 1.290e+02 2.100e+02 1.560e+02 1.640e+02 1.410e+02 1.140e+02\n",
            " 2.490e+02 1.250e+02 1.430e+02 1.340e+02 3.000e+00 1.590e+02 1.740e+02\n",
            " 1.480e+02 1.400e+02 1.080e+02 1.800e+02 1.650e+02 2.690e+02 2.530e+02\n",
            " 3.080e+02 1.830e+02 1.300e+02 1.700e+02 2.060e+02 2.000e+00 1.620e+02\n",
            " 1.200e+02 1.440e+02 1.460e+02 1.510e+02 1.810e+02 1.940e+02 1.980e+02\n",
            " 1.820e+02 1.550e+02 2.350e+02 1.680e+02 1.540e+02 1.950e+02 1.170e+02\n",
            " 1.600e+02 6.590e+02 2.650e+02 1.230e+02 3.150e+02 1.630e+02 1.320e+02\n",
            " 4.270e+02 2.130e+02 1.390e+02 4.490e+02 2.140e+02 2.390e+02 3.220e+02\n",
            " 1.500e+02 1.360e+02 2.180e+02 1.420e+02 1.910e+02 1.660e+02 1.490e+02\n",
            " 2.620e+02 1.970e+02 2.420e+02 2.050e+02 2.220e+02 2.309e+03 2.810e+02\n",
            " 2.150e+02 1.470e+02 2.300e+02 2.230e+02 2.710e+02 2.860e+02 3.170e+02\n",
            " 1.780e+02 3.980e+02 1.720e+02 2.950e+02 2.040e+02 2.450e+02 1.840e+02\n",
            " 2.640e+02 2.470e+02 3.650e+02 1.990e+02 3.000e+02 1.370e+02 3.200e+02\n",
            " 2.120e+02 2.090e+02 1.159e+03 1.920e+02 1.890e+02 4.960e+02 2.540e+02\n",
            " 1.580e+02 3.350e+02 1.330e+03 1.730e+02 2.960e+02 3.560e+02 2.340e+02\n",
            " 2.080e+03 2.190e+02 2.010e+02 3.660e+02 2.980e+02 3.120e+02 2.030e+02\n",
            " 3.070e+02 3.610e+02 4.430e+02 2.160e+02 3.300e+02 6.920e+02 1.770e+02\n",
            " 3.020e+02 1.000e+00 4.730e+02 1.710e+02 1.200e+03 1.850e+02 2.380e+02\n",
            " 2.890e+02 1.900e+02 1.750e+02 3.780e+02 1.880e+02 2.210e+02 8.380e+02\n",
            " 2.260e+02 2.610e+02 3.230e+02 2.830e+02 2.000e+02 2.560e+02 3.330e+02\n",
            " 3.240e+02 2.580e+02 2.480e+02 2.170e+02 3.590e+02 3.580e+02 2.550e+02\n",
            " 2.080e+02 4.980e+02 2.460e+02 3.030e+02 3.380e+02 1.760e+02 2.110e+02\n",
            " 6.400e+02 3.460e+02 2.840e+02 2.310e+02 2.590e+02 2.250e+02 3.130e+02\n",
            " 3.320e+02 3.340e+02 2.990e+02 3.820e+02 6.370e+02 2.430e+02 1.690e+02\n",
            " 9.130e+02 2.570e+02 3.540e+02 8.890e+02 3.900e+02 4.040e+02 3.360e+02\n",
            " 3.880e+02 3.180e+02 2.400e+02 5.410e+02 3.940e+02 2.200e+02 2.910e+02\n",
            " 3.250e+02 2.520e+02 6.100e+02 8.020e+02 2.850e+02 3.100e+02 3.040e+02\n",
            " 4.440e+02 2.290e+02 6.660e+02 3.290e+02 2.760e+02 5.670e+02 3.910e+02\n",
            " 2.770e+02 4.210e+02 2.440e+02 3.410e+02 3.140e+02 2.680e+02 3.270e+02\n",
            " 5.770e+02 4.470e+02 2.970e+02 1.960e+02 2.240e+02 4.810e+02 3.730e+02\n",
            " 4.330e+02 2.900e+02 4.570e+02 4.760e+02 2.730e+02 2.330e+02 3.770e+02\n",
            " 3.090e+02 5.890e+02 4.130e+02 3.310e+02 7.870e+02 2.940e+02 3.740e+02\n",
            " 4.070e+02 1.267e+03 2.280e+02 4.940e+02 3.160e+02 2.750e+02 4.690e+02\n",
            " 2.600e+02 5.200e+02 3.620e+02 2.700e+02 5.290e+02 5.610e+02 1.212e+03\n",
            " 5.280e+02 4.510e+02 3.990e+02 4.200e+02 5.310e+02 2.370e+02 5.240e+02\n",
            " 3.810e+02 8.290e+02 2.720e+02 4.160e+02 4.870e+02 6.870e+02 8.000e+02\n",
            " 3.260e+02 9.990e+02 8.300e+02 3.850e+02 6.900e+02 9.690e+02 3.860e+02\n",
            " 2.790e+02 4.900e+02 3.570e+02 4.370e+02 1.858e+03 4.860e+02 3.760e+02\n",
            " 3.430e+02 2.410e+02 3.470e+02 3.690e+02 4.380e+02 6.070e+02 4.540e+02\n",
            " 6.170e+02 1.274e+03 2.660e+02 2.880e+02 2.820e+02 3.520e+02 5.580e+02\n",
            " 2.740e+02 2.535e+03 1.940e+03 5.180e+02 3.970e+02 4.240e+02 3.600e+02\n",
            " 3.440e+02 4.600e+02 1.471e+03 2.630e+02 3.050e+02 5.510e+02 2.780e+02\n",
            " 2.500e+02 5.340e+02 4.010e+02 5.710e+02 1.221e+03 1.238e+03 5.990e+02\n",
            " 4.260e+02 5.830e+02 6.710e+02 2.030e+03 5.860e+02 9.810e+02 2.670e+02\n",
            " 3.750e+02 6.230e+02 5.690e+02 5.210e+02 2.930e+02 3.500e+02 5.570e+02\n",
            " 2.800e+02 2.510e+02 2.870e+02 3.720e+02 4.560e+02 7.020e+02 4.060e+02\n",
            " 3.920e+02 4.720e+02 7.440e+02 4.090e+02 4.450e+02 3.830e+02 4.790e+02\n",
            " 3.010e+02 3.630e+02 3.890e+02 1.016e+03 1.128e+03 7.490e+02 6.890e+02\n",
            " 6.860e+02 3.390e+02 2.088e+03 5.470e+02 4.250e+02 1.337e+03 4.950e+02\n",
            " 3.517e+03 3.450e+02 5.400e+02 3.800e+02 8.260e+02 3.550e+02 3.680e+02\n",
            " 5.420e+02 5.900e+02 4.190e+02 5.030e+02 2.920e+02 7.600e+02 8.330e+02\n",
            " 4.290e+02 3.840e+02 4.580e+02 6.160e+02 6.770e+02 3.490e+02 4.840e+02\n",
            " 1.336e+03 1.774e+03 1.351e+03 7.210e+03 1.052e+03 3.370e+02 1.830e+03\n",
            " 5.100e+02 9.860e+02 3.307e+03 4.830e+02 5.010e+02 1.390e+03 7.570e+02\n",
            " 4.880e+02 1.265e+03 4.050e+02 4.230e+02 5.880e+02 4.680e+02 3.060e+02\n",
            " 9.680e+02 9.750e+02 5.430e+02 4.120e+02 1.206e+03 3.400e+02 2.273e+03\n",
            " 5.460e+02 4.220e+02 2.530e+03 6.560e+02 6.610e+02 3.870e+02 7.220e+02\n",
            " 6.950e+02 8.740e+02 4.620e+02 4.180e+02 7.500e+02 5.370e+02 3.190e+02\n",
            " 5.680e+02 1.172e+03 4.310e+02 1.097e+03 7.000e+02 1.001e+03 9.100e+02\n",
            " 5.550e+02 9.380e+02 4.610e+02 1.216e+03 4.410e+02 3.807e+03 3.530e+02\n",
            " 4.633e+03 7.670e+02 7.070e+02 1.631e+03 5.110e+02 1.935e+03 5.740e+02\n",
            " 7.190e+02 2.981e+03 8.480e+02 8.370e+02 4.320e+02 5.160e+02 4.660e+02\n",
            " 6.880e+02 4.670e+02 5.060e+02 3.700e+02 1.522e+03 1.009e+03 4.150e+02\n",
            " 5.790e+02 4.590e+02 2.277e+03 2.698e+03 5.750e+02 4.080e+02 5.230e+02\n",
            " 7.390e+02 6.390e+02 4.000e+02 1.198e+03 4.640e+02 2.059e+03]\n",
            "\n",
            "Unique values in 'gamma_GTP':\n",
            "[ 40.  27.  68.  18.  25.  37.  12.  35.  26.  16.  19.  42.  31.  58.\n",
            "   9.  13.  60.  48.  32.  33.  15.  76.  29.  10.  53.  28.  45.  20.\n",
            "  62.  23.  93.  17.  22.  21. 278.  39.  14.  67.  49.  11. 292. 236.\n",
            "  47.  24.  52. 131.  83.  34.  50.  61. 160.  65.  36.  63.  38.  30.\n",
            " 151.  44.  86.  66. 179.   7. 180.  41.  77. 108.  70.  57.  56. 127.\n",
            " 119.  82.  46.  81.  43. 113. 122.  71. 143. 126.  64. 120.  55.  69.\n",
            "  51. 306.  72. 231. 107.  87.  59.   8. 256.   6. 189. 149.  98. 116.\n",
            " 133. 156.  54. 521.   3. 182. 111. 177.  73.  97.  95.  99. 513. 106.\n",
            " 196. 102.  75.  79. 125.   5. 176. 257.  96. 170.  78. 140.  74. 311.\n",
            "  90. 172. 150. 871.  84.  88. 538. 204.  92. 173. 123. 490. 186. 805.\n",
            " 193.  80. 146. 138.  94.  85. 389. 203. 147. 142. 100. 110. 145. 153.\n",
            " 284. 314. 117. 299. 999. 320. 217. 229. 188. 305. 429. 101. 178. 211.\n",
            " 260. 128.  89. 134. 184. 115. 112. 174. 192. 104. 195. 221. 287. 155.\n",
            " 187. 239. 171. 169. 562. 118. 162. 130. 161. 262. 342.  91. 105. 197.\n",
            " 270. 234. 255. 103. 166. 225. 167. 355. 226. 428. 144. 213. 374.   4.\n",
            " 109. 154. 165. 282. 148. 259. 212. 252. 599. 290. 273. 630. 201. 427.\n",
            " 157. 338. 420. 312. 202. 183. 137. 230. 359. 181. 617. 121. 208. 244.\n",
            " 425. 141. 424. 242. 310. 228. 158. 641. 296. 129. 135. 164. 248. 216.\n",
            " 506. 136. 356. 132. 307. 942. 330. 261. 124. 279. 280. 159. 316. 232.\n",
            " 286. 422. 210. 190. 114. 206. 249. 791. 467. 185. 139. 343. 549. 214.\n",
            " 411. 168. 459. 223. 313. 205. 269. 289. 209. 163. 265. 277. 472. 309.\n",
            " 175. 386. 317. 222. 655. 218. 328. 577. 250. 372. 663. 430. 293. 264.\n",
            " 453. 266. 198. 191. 345. 245. 194. 224. 240. 423. 272. 227. 247. 553.\n",
            " 349. 199. 294. 335. 449. 329. 847. 207. 325. 370. 518. 268. 322. 419.\n",
            " 377. 643. 387. 507. 488. 152. 624. 364. 283. 319. 455. 477. 291. 391.\n",
            " 402. 200. 215. 243. 220. 354. 281. 241. 341. 579. 253. 235. 333. 331.\n",
            " 725. 303. 451. 775. 486. 694. 501. 842. 297. 668. 401. 689. 740. 285.\n",
            " 434. 295. 251.   2. 497. 344. 371. 263. 326. 759. 276. 416. 574. 891.\n",
            " 323. 816. 233. 648. 763. 615. 246. 302. 839. 368. 304. 376. 569. 332.\n",
            " 366. 409. 408. 412. 605. 219. 405. 651. 822. 489. 667. 921. 375. 555.\n",
            " 275. 267. 727. 859. 595. 298. 348. 635. 352. 575. 481. 502. 421. 445.\n",
            " 308. 334. 498. 438. 444. 715. 439. 717. 433. 321. 760. 254. 380. 499.\n",
            " 873. 746. 340. 539. 597. 530. 336. 515. 454. 979. 739. 524. 565. 681.\n",
            " 476. 653. 926. 288. 889. 373. 768. 324. 972. 406. 959. 363. 383. 461.\n",
            " 882. 550. 450. 576. 779. 350. 384. 692. 675. 659. 353. 552. 743. 447.\n",
            " 991. 399. 351. 657. 656. 978. 690. 610. 367. 798. 339. 561. 726. 238.\n",
            " 388. 546. 396. 431. 361. 446. 478. 395. 456. 568. 381. 392. 315. 745.\n",
            " 619. 458. 394. 570. 369. 426. 540. 327. 758. 357. 795. 237. 931. 664.\n",
            " 258. 503. 713. 271. 385. 413. 403. 571. 770. 600. 469. 733. 442. 400.\n",
            " 390. 583. 362.   1. 300. 460. 517. 318. 346. 365. 418. 465. 410. 404.\n",
            " 415. 596. 301. 378. 878. 594. 677. 777. 676. 628. 468. 535. 407. 623.\n",
            " 925. 900. 817. 645. 441. 558. 393. 788. 436. 772. 971. 337. 586. 866.\n",
            " 622. 642. 753. 534. 274. 654. 732. 537. 850. 510. 443. 632. 905. 647.\n",
            " 955. 633. 720. 485. 998. 703. 749. 347. 483. 803. 382. 529. 572. 527.\n",
            " 686. 897. 693. 358. 432. 649. 578. 491. 731. 500. 508. 828. 474. 505.\n",
            " 774. 566. 943. 824. 683. 470. 601. 360. 493. 543. 701. 511. 606. 735.\n",
            " 836. 554. 820. 611. 646. 548. 564. 813. 519. 707. 964. 890. 588. 435.\n",
            " 616. 684. 464. 514. 522. 815. 448. 794. 638. 466. 977. 679. 864. 872.\n",
            " 523. 585. 902. 818. 457. 487. 750. 967. 714. 509. 709. 516. 800. 496.\n",
            " 398. 482. 479. 723. 604. 932. 492. 614. 580. 440. 437. 613. 924. 710.\n",
            " 660. 589. 417. 918. 495. 938. 629. 608. 716. 544. 940. 721. 782. 494.\n",
            " 662. 868. 769. 799. 463. 397. 625. 598. 678. 696. 894. 989. 591. 793.\n",
            " 379. 462. 823. 567. 851. 669. 560. 520. 841. 699. 612. 778. 947. 452.\n",
            " 846. 748. 626. 484. 950. 536. 551. 957. 867. 473. 414. 953. 920. 858.\n",
            " 547. 526. 865. 909. 786. 573. 528. 801. 584. 881. 888. 674. 831. 773.\n",
            " 545. 475. 874. 698. 837. 542. 914. 765. 992. 582. 532. 807. 639. 994.\n",
            " 908. 948. 658. 884. 541. 634. 910. 602. 590. 525. 747. 840. 670. 563.\n",
            " 833. 965. 827. 738. 756. 789. 640. 993. 533. 990. 937. 592. 904. 852.\n",
            " 621. 587. 819. 741. 650. 860. 593. 869. 951. 556. 697. 853. 618. 886.\n",
            " 711. 899. 906. 939. 688. 792. 712. 966. 806. 665. 512. 896. 752. 974.\n",
            " 734. 609. 581. 744. 861. 934. 809. 695. 927. 480. 504. 755. 718. 627.\n",
            " 702. 620. 700. 787. 907. 471. 781. 855. 811. 830. 736. 913. 751. 983.\n",
            " 704. 531. 705. 802. 814. 984. 559. 996. 728. 603. 875. 976. 637. 856.\n",
            " 919. 912. 652. 790. 729. 892. 825. 898. 935. 706. 607. 946. 783. 680.\n",
            " 761. 762. 970. 780. 986. 661. 785. 796. 771. 849. 933. 691. 687. 922.\n",
            " 557. 997. 848. 644. 730. 766. 742. 893. 666. 631. 724. 929. 672. 923.\n",
            " 903. 969. 737. 764. 754. 810. 835. 975. 911. 673. 917. 949. 821. 767.\n",
            " 636. 973.]\n",
            "\n",
            "Unique values in 'SMK_stat_type_cd':\n",
            "[1. 3. 2.]\n",
            "\n",
            "Unique values in 'DRK_YN':\n",
            "['Y' 'N']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL9u0JwaYVeU",
        "outputId": "2667bc7e-3650-419b-9429-5c9446dec64d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 991320 entries, 0 to 991345\n",
            "Data columns (total 24 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   sex               991320 non-null  int64  \n",
            " 1   age               991320 non-null  int64  \n",
            " 2   height            991320 non-null  int64  \n",
            " 3   weight            991320 non-null  int64  \n",
            " 4   waistline         991320 non-null  float64\n",
            " 5   sight_left        991320 non-null  float64\n",
            " 6   sight_right       991320 non-null  float64\n",
            " 7   hear_left         991320 non-null  float64\n",
            " 8   hear_right        991320 non-null  float64\n",
            " 9   SBP               991320 non-null  float64\n",
            " 10  DBP               991320 non-null  float64\n",
            " 11  BLDS              991320 non-null  float64\n",
            " 12  tot_chole         991320 non-null  float64\n",
            " 13  HDL_chole         991320 non-null  float64\n",
            " 14  LDL_chole         991320 non-null  float64\n",
            " 15  triglyceride      991320 non-null  float64\n",
            " 16  hemoglobin        991320 non-null  float64\n",
            " 17  urine_protein     991320 non-null  float64\n",
            " 18  serum_creatinine  991320 non-null  float64\n",
            " 19  SGOT_AST          991320 non-null  float64\n",
            " 20  SGOT_ALT          991320 non-null  float64\n",
            " 21  gamma_GTP         991320 non-null  float64\n",
            " 22  SMK_stat_type_cd  991320 non-null  float64\n",
            " 23  DRK_YN            991320 non-null  int64  \n",
            "dtypes: float64(19), int64(5)\n",
            "memory usage: 189.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the DataFrame\n",
        "df = df.copy()\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "df['sex'] = label_encoder.fit_transform(df['sex'])\n",
        "df['DRK_YN'] = label_encoder.fit_transform(df['DRK_YN'])\n",
        "\n",
        "# Select features and target variable\n",
        "x = df[['sex', 'age', 'height', 'weight','waistline', 'sight_left', 'sight_right',\n",
        "        'hear_left', 'hear_right', 'SBP', 'DBP', 'BLDS', 'tot_chole', 'HDL_chole',\n",
        "        'LDL_chole', 'triglyceride', 'hemoglobin', 'urine_protein',\n",
        "        'serum_creatinine', 'SGOT_AST', 'SGOT_ALT', 'gamma_GTP']]\n",
        "y = df[[\"SMK_stat_type_cd\"]]\n",
        "y2 = df[[\"DRK_YN\"]]\n",
        "\n",
        "# Display the selected features and target variable\n",
        "print(\"Selected Features (x):\")\n",
        "print(x.head())  # Display the first few rows of x\n",
        "print(\"\\nTarget Variable (y):\")\n",
        "print(y.head())  # Display the first few rows of y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkjYqytxYYCH",
        "outputId": "cf4e89b0-0106-40b1-c737-a1ce56f483c9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features (x):\n",
            "   sex  age  height  weight  waistline  sight_left  sight_right  hear_left  \\\n",
            "0    1   35     170      75       90.0         1.0          1.0        1.0   \n",
            "1    1   30     180      80       89.0         0.9          1.2        1.0   \n",
            "2    1   40     165      75       91.0         1.2          1.5        1.0   \n",
            "3    1   50     175      80       91.0         1.5          1.2        1.0   \n",
            "4    1   50     165      60       80.0         1.0          1.2        1.0   \n",
            "\n",
            "   hear_right    SBP  ...  tot_chole  HDL_chole  LDL_chole  triglyceride  \\\n",
            "0         1.0  120.0  ...      193.0       48.0      126.0          92.0   \n",
            "1         1.0  130.0  ...      228.0       55.0      148.0         121.0   \n",
            "2         1.0  120.0  ...      136.0       41.0       74.0         104.0   \n",
            "3         1.0  145.0  ...      201.0       76.0      104.0         106.0   \n",
            "4         1.0  138.0  ...      199.0       61.0      117.0         104.0   \n",
            "\n",
            "   hemoglobin  urine_protein  serum_creatinine  SGOT_AST  SGOT_ALT  gamma_GTP  \n",
            "0        17.1            1.0               1.0      21.0      35.0       40.0  \n",
            "1        15.8            1.0               0.9      20.0      36.0       27.0  \n",
            "2        15.8            1.0               0.9      47.0      32.0       68.0  \n",
            "3        17.6            1.0               1.1      29.0      34.0       18.0  \n",
            "4        13.8            1.0               0.8      19.0      12.0       25.0  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Target Variable (y):\n",
            "   SMK_stat_type_cd\n",
            "0               1.0\n",
            "1               3.0\n",
            "2               1.0\n",
            "3               1.0\n",
            "4               1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ReCheck which column needs fixing\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values in '{column}':\")\n",
        "    print(unique_values)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_dQEt37YZcf",
        "outputId": "0a7aac71-eac8-4c60-bc29-a3c4963875ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'sex':\n",
            "[1 0]\n",
            "\n",
            "Unique values in 'age':\n",
            "[35 30 40 50 45 55 65 25 60 20 70 75 80 85]\n",
            "\n",
            "Unique values in 'height':\n",
            "[170 180 165 175 150 155 160 145 140 185 135 190 130]\n",
            "\n",
            "Unique values in 'weight':\n",
            "[ 75  80  60  55  65  50  85  70  45  40  95 120  90  35 105 100 110 115\n",
            " 130  30 125 140  25 135]\n",
            "\n",
            "Unique values in 'waistline':\n",
            "[ 90.   89.   91.   80.   75.   69.   84.2  84.   82.   79.2  98.   72.3\n",
            "  88.   76.   73.   78.   99.   85.   67.   62.   92.   79.   87.   70.\n",
            "  67.5  87.3  71.   92.9  94.   79.3  77.   75.7  85.5  74.   60.   81.\n",
            "  72.   65.   63.   81.6  83.   61.  110.   86.8  73.5  93.  109.   54.\n",
            "  91.2  66.   79.5  86.   97.1  76.2  80.5  68.   64.   74.1  85.9  65.3\n",
            "  95.   94.5 100.   85.4  77.6  73.3 103.   93.5  67.8  69.2 105.7 105.\n",
            "  74.2  97.   75.4  83.2  88.5  85.3  87.4  71.5  64.1  76.6  93.1  84.8\n",
            "  88.1  66.8  96.   81.5  80.1  87.2  86.5 104.  114.   56.   88.8  89.2\n",
            "  66.2  90.8  88.2  82.5  65.4  72.2  81.3  75.6  87.8  77.2  98.5  85.2\n",
            "  97.5  63.5  95.4  72.6 110.8  81.2  87.5  89.5  82.4  81.8  76.5  87.1\n",
            "  78.1  72.7  84.3 101.6  84.5  98.7 101.   98.1  66.5  90.4  76.1  77.1\n",
            "  82.1  93.3  80.4  70.3  71.1 111.   90.1  70.6  65.1  83.8  81.4  68.1\n",
            " 106.   86.2  70.2 116.   83.3  69.8  58.5  84.7  74.5  71.4  86.7 104.2\n",
            " 112.   92.7 107.7  82.6  70.5  75.3 113.   59.   68.8  95.1  95.6 100.4\n",
            "  96.7  78.3  72.5  79.9 102.   92.2  91.8  73.2  79.8  90.6  95.7  77.8\n",
            "  60.5  70.4  96.2  82.2  81.1  86.3  80.8  94.2  66.4  63.2  63.7  68.5\n",
            " 101.8  93.8  96.1  97.2  79.6  91.3  83.5  65.5  99.9  78.8  58.  118.4\n",
            "  92.1  93.7  66.1  71.2  75.5 107.   68.2  87.7  78.4  94.1 115.   86.9\n",
            "  96.6  99.1 108.5  75.8  88.7  93.2  99.3  89.1  88.9  91.5  75.2  96.8\n",
            "  80.2  68.4  77.5  64.9  74.9  84.4  74.4  82.8  71.8  86.6  94.3  84.9\n",
            " 107.1  86.1  69.4  76.4  82.3  89.8 106.5  72.4 108.   87.6  80.7  90.3\n",
            "  73.4  78.2  92.6  74.6 105.5  65.9  78.5  94.6  95.2  64.5  91.4  62.5\n",
            "  83.6 108.2  69.5  90.2  93.4  81.7 122.4  86.4  88.3  99.2  85.6  75.9\n",
            "  83.1  61.5  98.4  64.8 118.   57.   76.9  68.6  63.6  82.9  92.5 128.\n",
            " 107.5  67.6 109.5  73.1  68.7  68.3 105.2  71.6  85.1  78.6  76.3  77.9\n",
            "  97.6  88.4  94.7  89.7  70.8 100.3  78.7  80.6  90.9  95.5  91.1  97.9\n",
            " 101.5  67.4  65.2  74.8  73.7  99.4  77.4  84.6  69.1  70.1  72.9  81.9\n",
            "  74.7 100.5  79.1  72.8  61.8  67.2  80.3  73.8  72.1 126.5 103.5  62.1\n",
            "  76.7  89.9  75.1 102.1  89.6  92.4 104.5  64.6  61.4  76.8  69.7 126.\n",
            "  62.7  92.3  59.2  96.5 103.3  84.1  74.3  59.7 104.8  97.4  80.9  73.9\n",
            " 109.2  83.7  89.4  90.5 103.2  65.8 101.1  89.3 105.6  69.3  51.   83.4\n",
            " 117.  111.4  94.4 105.4  98.9  92.8  82.7  99.5  70.9 106.2  79.4  73.6\n",
            "  79.7  83.9  99.7  61.6  88.6  55.   68.9  96.9  66.7  78.9 123.  103.1\n",
            "  94.8  91.9  95.8  95.3 102.6 103.6  77.3  85.8  62.8 111.8  62.2 105.1\n",
            "  62.4  87.9  63.9  93.9  99.8  57.1 101.3  66.6  67.9  49.   85.7 112.6\n",
            "  71.3  62.3  66.3  63.1  98.6  61.1  64.2  71.9  91.7  60.2  59.6  90.7\n",
            " 108.3  77.7 112.5 104.7  59.3  65.6 107.3  70.7  69.9  67.1  65.7  61.9\n",
            " 999.   61.2 102.5  67.3  63.8  91.6  97.3 109.7  98.3 100.1  57.7  94.9\n",
            " 112.2  98.8  56.7  55.4 106.9 113.4 100.7 119.  113.9  58.8  97.8  59.5\n",
            "  63.3  71.7 121.  108.9 108.6 116.4  64.7  63.4  64.3 110.9  95.9 100.2\n",
            "  60.1 102.2  67.7 109.6  96.3 108.7  98.2 102.3  56.1  64.4  93.6 122.\n",
            "  35.   58.2 120.  103.8 106.7 104.1 102.8 111.3  56.8 104.6 113.5 115.5\n",
            " 114.2  60.3  61.3  60.9  58.6 107.8 101.4 110.5  60.8 100.8 106.8  66.9\n",
            " 115.2 112.3 104.4 100.6 109.1  60.6  61.7 103.4  56.3  57.9  99.6  97.7\n",
            "  59.8 101.7 110.1 114.6  52.1  59.1  69.6 100.9 113.7  59.4  56.2 114.5\n",
            "  62.9  59.9  52.  110.4  96.4 107.4 123.8 109.3  57.8 107.6 119.5 102.4\n",
            " 111.5 110.7 117.2 120.7  53.  129.  103.7 104.3 101.2 102.7 104.9 112.8\n",
            " 122.5 114.7  62.6  58.7  55.6 109.8 105.8 106.1 117.7 108.1 113.8 109.4\n",
            " 118.5 105.3  55.5  54.4 101.9 125.   58.9  60.4  57.2 107.2 124.5 111.1\n",
            " 114.8  58.3 107.9  53.2  57.4 130.  145.   50.   55.8  60.7 106.3 106.6\n",
            " 110.3  54.3 111.2  56.9 113.1 136.  114.1 138.  102.9 124.  117.5  55.1\n",
            " 116.8 118.2 120.5 108.4  55.2  56.5 129.6 120.6 112.4 127.   58.4 109.9\n",
            " 116.5 116.3 124.2 121.4 132.  117.8 119.1 115.6 115.4 112.1  57.3 118.9\n",
            "  53.1  55.7  54.5  57.6 106.4  57.5 116.2 110.2 113.3 117.3  48.   27.\n",
            " 123.2 119.7 121.2  58.1 118.8 108.8 123.5 140.  103.9 112.7 120.1 119.2\n",
            "  52.6 105.9 113.2 118.1 115.8 120.2  53.4 111.7  56.4 130.5  55.9 110.6\n",
            "  42.   54.6 116.6 116.1 119.3  32.  114.3 121.3 117.9 134.   53.5  54.8\n",
            " 121.5  54.2  51.5  50.5 117.6  40.  115.3 114.9 117.1 126.6 119.8 115.7\n",
            "  52.5 115.9 119.4  43.   30.  133.  149.1 123.3 113.6  56.6  53.8  51.6\n",
            " 123.4  55.3  50.3 111.6 136.8  54.1 115.1 126.2  51.8 116.9 131.  135.\n",
            "  53.6 117.4  54.9  51.1 122.3   8.  121.1 116.7 125.5 111.9 127.3  52.4\n",
            " 122.6 124.1 123.1 118.3  51.2]\n",
            "\n",
            "Unique values in 'sight_left':\n",
            "[1.  0.9 1.2 1.5 0.5 0.7 0.3 0.6 0.8 0.2 9.9 0.4 0.1 2.  1.6 1.1 1.8 1.9\n",
            " 1.3 1.4 1.7 2.1 2.5 2.2]\n",
            "\n",
            "Unique values in 'sight_right':\n",
            "[1.  1.2 1.5 0.4 0.9 9.9 0.8 0.7 0.6 0.5 0.3 0.2 0.1 2.  1.6 1.4 1.8 1.1\n",
            " 1.9 1.7 1.3 2.1 2.5 2.2]\n",
            "\n",
            "Unique values in 'hear_left':\n",
            "[1. 2.]\n",
            "\n",
            "Unique values in 'hear_right':\n",
            "[1. 2.]\n",
            "\n",
            "Unique values in 'SBP':\n",
            "[120. 130. 145. 138. 142. 101. 132. 118. 109. 129. 113. 126. 119. 121.\n",
            " 111. 110. 122. 114. 115. 128. 133. 167. 127. 102. 137. 116. 108. 134.\n",
            " 131. 124. 100. 135. 136. 149. 146. 148. 123. 183. 140. 139.  90. 104.\n",
            " 112. 105. 106. 150. 184. 141.  94.  93. 125. 163. 103. 107.  99. 155.\n",
            " 151. 117. 180. 195. 144.  96.  98.  95. 143. 153. 158.  84. 156. 165.\n",
            " 152.  97. 172. 171. 160.  92. 147. 164.  91. 162. 154. 161. 157.  85.\n",
            " 168. 187. 170. 159.  82.  86. 188.  87.  89. 166.  88. 178. 175. 169.\n",
            " 194. 173. 190. 177. 230. 174.  75. 176. 229.  83. 203. 193.  80. 179.\n",
            "  79. 186. 181. 185. 204. 191. 182. 200. 207. 217. 192. 199. 206. 197.\n",
            " 202. 198. 210. 196.  77.  81. 212. 189. 208. 220.  78. 215. 232. 214.\n",
            " 219. 218. 201.  76. 241.  72. 236. 221. 224.  70. 211. 234. 222. 216.\n",
            " 270. 209.  74. 255. 205.  73. 240. 253. 244. 213. 235. 227. 223. 238.\n",
            " 228. 273.  67.]\n",
            "\n",
            "Unique values in 'DBP':\n",
            "[ 80.  82.  70.  87.  92.  58.  85. 105.  69.  72.  77.  78.  67.  74.\n",
            "  65.  76.  64.  55.  75.  83.  84. 102.  60.  62.  86.  90.  71.  94.\n",
            " 109.  73.  79.  66.  56.  63.  57.  81.  52.  68.  93.  89. 104.  95.\n",
            "  88. 100. 115.  53.  91.  61. 106.  97.  59.  54.  96.  51. 111. 112.\n",
            "  98. 110. 103.  99. 120. 134. 108. 114. 101. 123. 113.  47.  50. 130.\n",
            " 160. 119.  46. 107. 118. 126.  49.  45. 124. 117. 116. 135.  42. 140.\n",
            " 127. 125. 132.  48. 129. 128. 122.  43. 121. 133. 131.  44. 142.  41.\n",
            " 137.  36. 138. 150. 141. 145.  38. 163. 185.  37. 146. 143. 144. 139.\n",
            "  40.  39. 153. 154.  33. 156. 164. 181. 136.  34. 180. 149. 147. 170.\n",
            "  32.]\n",
            "\n",
            "Unique values in 'BLDS':\n",
            "[ 99. 106.  98.  95. 101.  89.  94. 104. 100.  90. 137.  82.  79.  96.\n",
            " 105.  88. 111. 102.  87.  83.  81.  91. 110.  93. 128. 103.  86.  80.\n",
            "  97.  85.  84. 112. 118. 155. 160. 114. 120. 107. 109. 140. 161. 119.\n",
            " 125.  92.  76. 324.  75.  77. 121.  73. 108.  78. 124. 192. 115.  74.\n",
            " 113. 217. 229.  68. 425.  70. 163.  69. 130.  71. 143. 212. 133. 129.\n",
            " 117. 144. 168. 162. 127. 193. 220. 138. 249. 200. 151. 169. 150. 116.\n",
            " 146. 148. 235. 136. 134.  62. 126. 135. 184. 123.  72. 290.  65. 153.\n",
            " 122. 142. 174. 131. 147. 154.  63. 164. 149. 139. 141. 306.  67. 181.\n",
            "  66. 187. 175. 152. 190. 238. 132. 159. 180.  61. 156. 277. 215. 209.\n",
            " 210. 145. 158. 205. 194. 266. 204. 239. 421. 244. 172. 214. 240. 195.\n",
            " 170. 257. 157. 271. 211. 346. 412. 268. 236. 207. 231. 177.  60. 196.\n",
            " 253. 247. 373. 191. 319. 304. 242. 213.  58. 241. 167. 185. 178. 293.\n",
            " 222. 173. 186. 246. 230. 298. 259. 243. 182. 300. 228. 166. 468. 171.\n",
            " 197. 302. 202. 176. 198. 337. 270. 419. 426. 219. 188.  64. 183. 296.\n",
            " 258. 370. 165. 232. 179. 227. 262. 255. 305. 223. 225. 322. 272. 252.\n",
            " 437. 339. 310. 201. 299. 208. 380. 250. 224. 189. 206. 233. 216. 199.\n",
            " 312. 343. 367. 218. 327. 264. 254. 269. 320. 203. 273. 280. 283. 308.\n",
            " 342. 234. 303. 261.  54. 251. 263.  55. 334. 278. 329. 307.  51. 260.\n",
            " 245. 221. 274. 333. 311. 287. 284. 297.  56. 341. 248. 286. 321. 309.\n",
            "  59. 275. 279. 288. 291. 281. 357. 364. 316. 336. 318.  52. 267. 276.\n",
            " 328. 372. 317. 292. 301. 285. 400. 331. 500. 447. 356. 536. 433. 289.\n",
            " 314. 596. 282.  50. 402. 256. 226. 349.  47. 313. 332. 456. 265. 578.\n",
            " 294. 386. 429. 295.  42.  48. 381.  57. 237. 368. 360. 353. 385. 411.\n",
            " 685. 344. 428. 355.  46. 407. 629. 394.  45. 326. 551. 315. 351. 383.\n",
            "  38. 363. 338. 398. 323. 335. 491. 352. 340. 616. 358. 330. 393. 586.\n",
            " 354. 365. 378. 452. 325. 374. 348. 424. 396. 415. 377. 382. 480.  49.\n",
            " 470. 663. 439. 345. 469. 390. 359. 403. 541. 569. 505. 560. 369. 467.\n",
            " 347. 409. 401. 538. 462. 350. 769. 463. 392.  44. 418. 375. 427. 579.\n",
            " 477. 362. 379. 423. 446.  32. 438. 366. 458. 577. 454. 434. 406. 376.\n",
            " 502. 404. 599. 564.  53. 598.  43. 450. 410. 460. 483. 497. 457. 361.\n",
            " 459. 499. 597. 405. 391. 617.  30. 397. 443. 444.  36. 482. 461. 430.\n",
            " 420. 431. 440. 558. 414. 552.  33. 517. 408. 387. 453.  39. 516. 371.\n",
            " 384. 478. 535. 416. 513. 395. 512. 511. 464. 627.  34. 800.  37. 455.\n",
            " 493. 472. 801. 413. 496. 484. 389. 487. 503. 442. 476. 435. 561. 445.\n",
            " 436. 471. 522. 611. 588. 638. 486. 481. 784.  40. 449. 595. 388. 546.\n",
            " 852.  25. 741. 563. 495. 498. 527. 548.]\n",
            "\n",
            "Unique values in 'tot_chole':\n",
            "[ 193.  228.  136.  201.  199.  218.  196.  185.  217.  195.  183.  115.\n",
            "  200.  205.  113.  148.  147.  180.  197.  174.  207.  170.  156.  154.\n",
            "  293.  231.  246.  191.  212.  162.  208.  210.  232.  142.  181.  240.\n",
            "  209.  137.  241.  153.  219.  235.  263.  215.  224.  245.  167.  203.\n",
            "  155.  175.  315.  211.  250.  273.  277.  169.  233.  134.  135.  242.\n",
            "  150.  198.  223.  225.  244.  213.  165.  116.  172.  149.  262.  110.\n",
            "  214.  247.  179.  187.  152.  192.  168.  220.  272.  264.  138.  140.\n",
            "  161.  157.  186.  189.  230.  226.  158.  132.   95.  143.  299.  243.\n",
            "  280.  257.  236.  190.  222.  206.  278.  194.  202.  287.  144.  204.\n",
            "  159.  121.  163.  164.  166.  234.  151.  221.  160.  258.  129.  274.\n",
            "  259.  239.   90.  177.  141.  188.  184.  178.  384.  182.  290.  291.\n",
            "  265.  216.  268.  251.  119.  227.  139.  104.  100.  252.  173.  229.\n",
            "  124.  248.  130.  283.  102.  146.  171.  254.  238.  237.  275.  276.\n",
            "  285.  117.  270.  253.  266.  269.  145.  176.  284.  126.  131.  122.\n",
            "  103.  256.  282.  111.  133.  123.  255.  249.  267.  292.  307.  279.\n",
            "  320.  281.  306.  127.  288.  294.  261.  260.  303.  109.  289.  335.\n",
            "  271.  310.  304.  337.  308.  301.  125.  323.  311.  305.   98.  118.\n",
            "  105.  112.  369.  120.  286.  331.  128.  106.  114.  302.  309.  367.\n",
            "  348.  317.  295.   77.  312.  107.  316.  108.  298.  352.  313.  101.\n",
            "  349.  296.   73.  334.  322.  357.  326.  297. 1619.   91.  400.  600.\n",
            "   87.  330.   83.  343.  319.   94.   96.  338.   97.  324.  360.  346.\n",
            "  377.  300.  336.  314.  339.   86.  342.   57.  318.   54.  350.   84.\n",
            "  362.  321.  386.  325.  333.  395.  372.  358.   75.  426.  373.  329.\n",
            "  355.   92.  347.  344.  356.  327.  353.   82.  370.   88.  404.  405.\n",
            "   99.   85.  340.  328.  383.  413.  332.  700.  351.  398.  354.  500.\n",
            "  428.  345.   89.  361.  363.   81.  387.  388.  492.   93.  406.  412.\n",
            "  341.  423.  464.   78.  366.  567.  633.   76.  420.  452. 1030.  374.\n",
            "  368.  418.  650.  433.  380.  393.  474.  359.  468.   80.  365.  376.\n",
            "  407.  397.  394.  559.  408.   72.   69.  403.  415.  390.  424.  382.\n",
            "  656.  599.  483.  385.  744.  715. 1196.  409.   62.  391.  389.  441.\n",
            "  642.  435.  414.  482.  364.  506.  479.   68.  447.  442.   74.   60.\n",
            "  416.  450.  429.  730.  478.  379.  460.  417.  440.  401.  378.  725.\n",
            "  511.  475.  410.  439.  411.  371. 2196.  524.  514.  392.  532.   63.\n",
            "  396.  381.   71.  462.   64.  472.  453.   70.  431.  497.   79.  375.\n",
            "  607.  502. 1575. 2046.  399.  430. 2067.  485.  422. 1815. 1605.  563.\n",
            "  508.  489.  753.  557.  470.  710.  454.  438. 1736.  451.   58.  436.\n",
            "  432.  458.  522.  556.   30.  535. 1446.  427.  604.  523.  515.  480.\n",
            "  621.  837.  484.  421.   45.  445.  564.   55.   59.  662.  623.  419.\n",
            "   65. 2344.  446.  491.  465.  456.  897.  469.  473. 1306. 1315.  574.\n",
            " 2033.  792. 1335. 1191.   67.  467.]\n",
            "\n",
            "Unique values in 'HDL_chole':\n",
            "[4.800e+01 5.500e+01 4.100e+01 7.600e+01 6.100e+01 7.700e+01 6.600e+01\n",
            " 5.800e+01 5.600e+01 6.000e+01 4.200e+01 3.100e+01 5.100e+01 5.300e+01\n",
            " 4.400e+01 5.400e+01 4.300e+01 6.200e+01 3.900e+01 5.700e+01 5.900e+01\n",
            " 3.500e+01 2.900e+01 8.500e+01 5.000e+01 4.700e+01 1.000e+02 7.000e+01\n",
            " 4.600e+01 6.300e+01 4.500e+01 8.700e+01 6.400e+01 7.200e+01 6.700e+01\n",
            " 7.100e+01 6.900e+01 6.500e+01 4.000e+01 3.200e+01 4.900e+01 3.800e+01\n",
            " 8.400e+01 7.300e+01 9.000e+01 3.700e+01 2.800e+01 6.800e+01 3.400e+01\n",
            " 8.300e+01 7.400e+01 5.200e+01 8.600e+01 8.000e+01 7.900e+01 1.190e+02\n",
            " 7.500e+01 1.180e+02 3.300e+01 9.900e+01 9.300e+01 8.800e+01 9.100e+01\n",
            " 9.200e+01 8.200e+01 3.600e+01 7.800e+01 8.100e+01 1.030e+02 9.600e+01\n",
            " 9.400e+01 1.050e+02 9.800e+01 2.200e+01 1.070e+02 3.000e+01 1.120e+02\n",
            " 9.700e+01 9.500e+01 8.900e+01 1.110e+02 1.080e+02 1.200e+02 1.040e+02\n",
            " 2.600e+01 2.700e+01 1.130e+02 2.300e+01 1.060e+02 1.010e+02 1.100e+02\n",
            " 2.500e+01 1.020e+02 1.090e+02 4.000e+00 2.100e+01 1.640e+02 2.400e+01\n",
            " 1.570e+02 1.230e+02 1.150e+02 8.110e+03 1.240e+02 4.140e+02 1.900e+01\n",
            " 6.000e+00 1.340e+02 1.450e+02 1.260e+02 1.370e+02 1.270e+02 1.400e+02\n",
            " 1.160e+02 1.100e+01 2.000e+01 5.260e+02 1.170e+02 1.140e+02 1.310e+02\n",
            " 1.800e+01 1.220e+02 1.250e+02 1.350e+02 1.280e+02 1.300e+02 7.000e+00\n",
            " 1.700e+01 1.290e+02 3.000e+00 1.550e+02 1.210e+02 1.300e+01 1.400e+01\n",
            " 1.410e+02 1.430e+02 1.530e+02 1.360e+02 1.380e+02 2.000e+00 1.390e+02\n",
            " 1.500e+01 1.890e+02 5.660e+02 2.270e+02 1.830e+02 4.270e+02 1.480e+02\n",
            " 2.010e+02 1.600e+01 1.320e+02 3.050e+02 1.580e+02 1.510e+02 1.700e+02\n",
            " 1.200e+01 1.610e+02 1.420e+02 5.200e+02 1.460e+02 1.000e+01 1.470e+02\n",
            " 1.500e+02 4.760e+02 1.660e+02 5.350e+02 4.840e+02 1.520e+02 1.590e+02\n",
            " 9.000e+00 1.490e+02 1.000e+00 1.330e+02 1.440e+02 4.770e+02 1.770e+02\n",
            " 1.710e+02 4.090e+02 1.960e+02 1.630e+02 7.970e+02 3.530e+02 4.750e+02\n",
            " 6.360e+02 1.206e+03 5.000e+00 1.600e+02 7.010e+02 5.230e+02 1.800e+02\n",
            " 1.540e+02 1.900e+02 5.420e+02 1.650e+02 6.970e+02 8.000e+00 6.770e+02\n",
            " 5.920e+02 5.650e+02 1.760e+02 1.780e+02 2.090e+02 4.150e+02 1.920e+02\n",
            " 6.580e+02 1.560e+02 4.300e+02 2.220e+02 1.620e+02 4.470e+02 3.540e+02\n",
            " 5.250e+02 1.680e+02 9.330e+02 5.080e+02 3.670e+02 6.180e+02 5.700e+02\n",
            " 4.410e+02 7.270e+02 1.810e+02 1.820e+02 1.870e+02 2.120e+02]\n",
            "\n",
            "Unique values in 'LDL_chole':\n",
            "[1.260e+02 1.480e+02 7.400e+01 1.040e+02 1.170e+02 9.500e+01 1.150e+02\n",
            " 1.070e+02 1.410e+02 1.180e+02 1.300e+02 5.700e+01 8.900e+01 1.290e+02\n",
            " 1.080e+02 6.200e+01 8.200e+01 8.500e+01 1.030e+02 1.110e+02 9.800e+01\n",
            " 1.340e+02 6.400e+01 1.130e+02 8.600e+01 2.150e+02 1.520e+02 1.000e+02\n",
            " 1.500e+02 9.400e+01 1.510e+02 1.020e+02 1.100e+02 7.200e+01 1.460e+02\n",
            " 1.280e+02 1.010e+02 7.700e+01 1.190e+02 1.630e+02 1.780e+02 1.430e+02\n",
            " 1.270e+02 1.530e+02 1.380e+02 1.050e+02 2.110e+02 1.360e+02 1.060e+02\n",
            " 1.650e+02 1.710e+02 1.950e+02 1.560e+02 1.420e+02 7.100e+01 9.200e+01\n",
            " 6.800e+01 1.310e+02 1.640e+02 6.100e+01 8.800e+01 1.400e+02 1.330e+02\n",
            " 1.240e+02 1.470e+02 5.100e+01 1.450e+02 7.900e+01 5.300e+01 9.900e+01\n",
            " 8.000e+01 1.140e+02 9.100e+01 1.700e+02 1.540e+02 4.800e+01 9.600e+01\n",
            " 1.550e+02 1.320e+02 7.800e+01 9.000e+01 1.220e+02 1.890e+02 1.830e+02\n",
            " 5.400e+01 8.300e+01 6.500e+01 6.700e+01 1.210e+02 2.600e+01 1.660e+02\n",
            " 8.400e+01 1.670e+02 9.700e+01 1.990e+02 1.390e+02 5.000e+01 1.810e+02\n",
            " 1.160e+02 1.250e+02 1.730e+02 1.590e+02 1.740e+02 2.020e+02 1.370e+02\n",
            " 1.570e+02 2.210e+02 1.090e+02 4.100e+01 7.600e+01 3.600e+01 1.350e+02\n",
            " 1.120e+02 8.100e+01 1.840e+02 5.200e+01 1.620e+02 1.580e+02 2.900e+01\n",
            " 4.500e+01 1.200e+02 7.000e+01 5.800e+01 1.490e+02 3.250e+02 4.700e+01\n",
            " 1.440e+02 1.980e+02 1.900e+02 5.500e+01 9.300e+01 7.300e+01 1.230e+02\n",
            " 4.000e+01 4.900e+01 3.300e+01 6.900e+01 6.600e+01 5.900e+01 1.760e+02\n",
            " 2.500e+01 8.700e+01 7.500e+01 1.690e+02 6.300e+01 1.820e+02 1.600e+02\n",
            " 4.400e+01 4.300e+01 1.790e+02 2.240e+02 6.000e+01 5.600e+01 3.800e+01\n",
            " 1.940e+02 1.750e+02 1.770e+02 1.720e+02 4.200e+01 1.680e+02 1.860e+02\n",
            " 1.800e+02 1.880e+02 1.930e+02 2.170e+02 2.060e+02 1.960e+02 2.000e+02\n",
            " 3.000e+01 1.910e+02 2.040e+02 2.090e+02 1.850e+02 1.610e+02 1.400e+01\n",
            " 3.900e+01 1.700e+01 1.920e+02 3.500e+01 2.080e+02 2.160e+02 2.260e+02\n",
            " 2.700e+01 4.600e+01 2.650e+02 2.070e+02 2.360e+02 1.870e+02 2.190e+02\n",
            " 2.180e+02 2.130e+02 2.440e+02 3.700e+01 2.230e+02 2.200e+02 2.710e+02\n",
            " 2.410e+02 2.100e+02 2.100e+01 1.970e+02 3.200e+01 8.000e+00 2.010e+02\n",
            " 3.400e+01 2.050e+02 1.000e+01 2.030e+02 2.280e+02 2.200e+01 3.100e+01\n",
            " 2.120e+02 1.300e+01 2.000e+01 2.490e+02 2.340e+02 1.800e+01 2.330e+02\n",
            " 2.430e+02 2.290e+02 2.250e+02 5.119e+03 1.200e+01 2.140e+02 2.570e+02\n",
            " 1.600e+01 2.400e+01 2.450e+02 2.300e+01 2.370e+02 2.610e+02 2.420e+02\n",
            " 2.220e+02 6.000e+00 1.000e+00 2.760e+02 2.300e+02 2.460e+02 2.470e+02\n",
            " 2.520e+02 2.390e+02 2.810e+02 2.320e+02 2.350e+02 2.800e+01 7.000e+00\n",
            " 2.620e+02 2.780e+02 2.270e+02 1.100e+01 2.310e+02 1.900e+01 2.600e+02\n",
            " 2.400e+02 9.000e+00 2.730e+02 2.840e+02 2.820e+02 2.000e+00 2.480e+02\n",
            " 2.940e+02 2.910e+02 2.380e+02 3.220e+02 2.510e+02 2.590e+02 2.950e+02\n",
            " 3.210e+02 1.500e+01 2.530e+02 2.630e+02 2.550e+02 2.640e+02 3.100e+02\n",
            " 3.500e+02 2.880e+02 3.190e+02 2.900e+02 5.000e+00 3.090e+02 2.770e+02\n",
            " 2.700e+02 3.230e+02 2.850e+02 2.540e+02 3.390e+02 2.690e+02 2.500e+02\n",
            " 4.000e+00 4.040e+02 3.310e+02 1.660e+03 2.660e+02 2.790e+02 2.890e+02\n",
            " 1.750e+03 2.970e+02 3.030e+02 3.000e+00 2.960e+02 3.050e+02 2.680e+02\n",
            " 3.530e+02 3.930e+02 1.410e+03 6.510e+02 2.860e+02 2.740e+02 4.670e+02\n",
            " 2.580e+02 3.020e+02 3.240e+02 1.484e+03 2.920e+02 3.000e+02 3.380e+02\n",
            " 7.020e+02 3.040e+02 5.810e+02 1.126e+03 3.180e+02 2.750e+02 3.900e+02\n",
            " 5.000e+02 4.320e+02 6.710e+02 8.800e+02 2.670e+02 1.182e+03 3.120e+02\n",
            " 3.860e+02 5.630e+02 3.160e+02 2.800e+02 3.070e+02 3.420e+02 2.720e+02\n",
            " 3.590e+02 2.560e+02 1.044e+03 2.830e+02 2.870e+02 2.980e+02 3.440e+02\n",
            " 2.990e+02 3.510e+02 3.110e+02 3.130e+02 3.280e+02 3.970e+02 2.114e+03\n",
            " 4.010e+02 3.600e+02 3.200e+02 6.430e+02 3.080e+02 3.690e+02 3.170e+02\n",
            " 3.610e+02 3.840e+02 7.590e+02 4.160e+02 1.541e+03 2.026e+03 3.140e+02\n",
            " 8.540e+02 2.043e+03 3.770e+02 4.130e+02 1.798e+03 1.580e+03 3.430e+02\n",
            " 1.128e+03 4.710e+02 6.870e+02 4.820e+02 3.740e+02 3.880e+02 1.696e+03\n",
            " 3.300e+02 3.460e+02 5.490e+02 4.200e+02 3.360e+02 3.830e+02 3.800e+02\n",
            " 8.100e+02 4.440e+02 3.330e+02 2.930e+02 3.270e+02 1.425e+03 3.410e+02\n",
            " 3.150e+02 6.890e+02 3.010e+02 8.160e+02 1.461e+03 4.360e+02 7.400e+02\n",
            " 7.320e+02 3.940e+02 9.630e+02 1.200e+03 3.480e+02 3.550e+02 3.540e+02\n",
            " 3.350e+02 7.700e+02 2.254e+03 2.111e+03 5.370e+02 3.750e+02 7.800e+02\n",
            " 4.960e+02 3.260e+02 3.620e+02 6.200e+02 4.510e+02 3.630e+02 4.430e+02\n",
            " 4.310e+02 4.110e+02 1.293e+03 3.290e+02 1.298e+03 3.340e+02 3.680e+02\n",
            " 1.933e+03 1.311e+03 3.650e+02 1.476e+03 9.850e+02]\n",
            "\n",
            "Unique values in 'triglyceride':\n",
            "[  92.  121.  104. ... 1448. 1932. 1638.]\n",
            "\n",
            "Unique values in 'hemoglobin':\n",
            "[17.1 15.8 17.6 13.8 12.3 14.4 15.1 13.9 12.9 16.5 13.1 15.7 14.5 16.\n",
            " 14.8 15.2 12.1 16.7 12.5 16.1 15.9 12.8 14.3 15.  14.6 15.3 11.4 15.4\n",
            " 14.  10.8 15.6 11.8 13.2 13.4 12.6 13.6 13.5 17.2 16.4 12.4 12.2 14.1\n",
            " 17.  14.9 14.2 10.3 13.7 18.1 11.2 11.7 14.7 13.3 16.3 17.8 12.7 12.\n",
            " 11.5 10.5 15.5 10.1 10.2 16.2 13.  16.9 10.7 17.9  9.8 16.6 17.4 11.6\n",
            " 11.1 17.7 11.9 11.   9.3 16.8 10.6 11.3 10.   9.6 18.   9.5 17.3 18.4\n",
            "  7.2 17.5  9.2  8.8  9.7 10.9  8.3 19.8 10.4  9.   7.5 19.3  7.4 18.8\n",
            "  8.9  8.7  9.1  9.9  9.4  6.5 18.7 18.6  7.9 18.2  8.4  6.2 18.3 18.5\n",
            "  8.2 20.3  8.6  8.   8.1  8.5  7.7  7.1  7.8  7.6  6.1  6.6  7.3  6.7\n",
            " 20.2 19.4 18.9  7.  19.1  6.9  6.4  6.8 19.   6.3  1.   5.5 19.2  4.7\n",
            " 20.9 19.5 21.  19.9  5.9 23.9 19.7 20.8 19.6  4.4  5.7  4.3  5.8  4.1\n",
            "  4.9  5.4  3.9  5.3 24.2  5.1  6.  20.1  5.6  5.  20.7  3.7 21.3  5.2\n",
            "  4.2 20.5  4.  20.  22.7 22.  25.  21.7  4.8 21.2 21.6 22.1 20.6  3.8\n",
            "  4.5 23.6  2.8 23.3 21.5  4.6 21.8 21.1]\n",
            "\n",
            "Unique values in 'urine_protein':\n",
            "[1. 3. 2. 4. 5. 6.]\n",
            "\n",
            "Unique values in 'serum_creatinine':\n",
            "[ 1.   0.9  1.1  0.8  1.3  0.6  0.5  1.2  0.7  0.4  0.2  1.4  1.7  5.\n",
            "  3.   0.3  4.6  1.5  1.9  1.6  9.5  5.3  1.8 10.   2.4  8.   3.3  2.9\n",
            "  2.   0.1  2.1  4.5  6.4  2.2 16.4  2.7  7.   2.3  4.2  7.8 11.5  5.9\n",
            "  7.9  7.3 11.3  5.4 23.   2.6 15.7 11.6  2.5  8.1 13.9  6.   3.9 10.9\n",
            "  8.7 12.7  6.9  7.4  3.1 14.2 13.4 10.8  6.5  4.4 32.   6.1  4.8 12.\n",
            "  3.7  3.4 81.  37.   5.6  3.8  2.8 12.5  9.4  6.2 14.3  5.7  3.6  3.2\n",
            "  4.9  5.1  4.3  5.5  5.2  8.4  9.6  9.3  3.5  6.8  9.8  6.3 21.  94.\n",
            " 11.7  8.2 10.5 12.8  9.1 13.  76.  10.3  4.7 10.4  4.1  8.5 14.1  8.6\n",
            " 10.2 12.3  7.6  9.2 85.   7.7 14.   9.  11.9  8.9  7.5 18.5  6.6  7.2\n",
            "  4.   8.8 11.4 95.   8.3  5.8 15.9 13.1  9.9 14.4 12.9 11.2 13.7 16.6\n",
            " 18.1  7.1  9.7 11.  12.4  6.7 80.  10.7 11.8 13.8 24.4 19.  87.  93.\n",
            " 16.7 14.5 96.  98.  18.8 14.7 78.  16.8 12.6 75.  35.  20.  11.1 15.4\n",
            " 10.6 68.  26.  22.  13.3 12.1 13.2 79.  19.6 41.  15.5 39.  29.  12.2\n",
            " 10.1]\n",
            "\n",
            "Unique values in 'SGOT_AST':\n",
            "[2.100e+01 2.000e+01 4.700e+01 2.900e+01 1.900e+01 1.800e+01 3.200e+01\n",
            " 4.800e+01 1.300e+01 1.500e+01 1.200e+01 3.300e+01 4.100e+01 2.800e+01\n",
            " 1.700e+01 1.600e+01 3.100e+01 3.900e+01 2.500e+01 4.300e+01 2.200e+01\n",
            " 3.800e+01 3.000e+01 1.400e+01 2.300e+01 3.400e+01 4.600e+01 2.400e+01\n",
            " 2.700e+01 1.100e+01 6.700e+01 2.600e+01 4.000e+01 3.500e+01 4.900e+01\n",
            " 5.900e+01 3.600e+01 4.200e+01 4.400e+01 5.200e+01 1.410e+02 3.700e+01\n",
            " 6.600e+01 5.000e+01 5.500e+01 4.500e+01 4.000e+00 9.100e+01 5.100e+01\n",
            " 8.300e+01 5.600e+01 6.800e+01 7.500e+01 2.780e+02 1.000e+01 5.400e+01\n",
            " 6.900e+01 1.060e+02 1.000e+02 1.570e+02 6.000e+01 6.200e+01 7.400e+01\n",
            " 5.700e+01 5.000e+00 2.200e+02 7.900e+01 1.580e+02 9.000e+01 5.300e+01\n",
            " 7.800e+01 1.030e+02 1.330e+02 8.800e+01 6.500e+01 8.000e+01 8.600e+01\n",
            " 5.800e+01 1.510e+02 1.530e+02 9.300e+01 7.200e+01 9.400e+01 7.700e+01\n",
            " 1.670e+02 7.300e+01 6.100e+01 1.230e+02 8.400e+01 9.200e+01 8.200e+01\n",
            " 2.240e+02 7.000e+01 1.450e+02 7.600e+01 6.300e+01 6.400e+01 6.000e+00\n",
            " 1.270e+02 9.500e+01 9.700e+01 9.000e+00 8.000e+00 9.110e+02 8.700e+01\n",
            " 1.900e+02 7.100e+01 1.930e+02 1.440e+02 1.420e+02 1.470e+02 1.050e+02\n",
            " 1.070e+02 1.110e+02 8.500e+01 1.220e+02 1.560e+02 1.380e+02 8.100e+01\n",
            " 1.240e+02 1.200e+02 9.600e+01 1.320e+02 1.090e+02 1.020e+02 2.370e+02\n",
            " 2.770e+02 2.180e+02 1.490e+02 1.080e+02 1.150e+02 1.820e+02 2.640e+02\n",
            " 1.160e+02 2.920e+02 1.180e+02 1.210e+02 3.070e+02 1.300e+02 1.140e+02\n",
            " 1.290e+02 1.010e+02 1.250e+02 7.000e+00 8.900e+01 1.460e+02 2.510e+02\n",
            " 1.610e+02 1.880e+02 2.080e+02 1.810e+02 9.800e+01 1.130e+02 1.350e+02\n",
            " 1.680e+02 2.130e+02 9.900e+01 1.360e+02 1.120e+02 1.800e+02 1.040e+02\n",
            " 1.310e+02 1.190e+02 2.950e+02 1.690e+02 1.260e+02 2.170e+02 1.390e+02\n",
            " 3.230e+02 3.560e+02 9.240e+02 2.060e+02 2.620e+02 1.990e+02 2.010e+02\n",
            " 1.860e+02 2.740e+02 2.400e+02 3.000e+00 1.100e+02 4.540e+02 1.540e+02\n",
            " 1.280e+02 1.500e+02 1.660e+02 1.650e+02 3.710e+02 1.740e+02 1.730e+02\n",
            " 7.250e+02 1.590e+02 2.000e+02 2.460e+02 2.210e+02 2.150e+02 1.170e+02\n",
            " 1.400e+02 1.340e+02 8.360e+02 3.810e+02 7.120e+02 2.990e+02 1.911e+03\n",
            " 1.760e+02 2.880e+02 1.790e+02 2.050e+02 1.840e+02 1.870e+03 3.970e+02\n",
            " 1.480e+02 1.600e+02 1.750e+02 1.520e+02 3.910e+02 1.000e+00 6.230e+02\n",
            " 2.090e+02 2.100e+02 7.970e+02 1.780e+02 3.140e+02 1.550e+02 1.770e+02\n",
            " 3.460e+02 2.680e+02 5.050e+02 3.370e+02 2.410e+02 1.960e+02 1.430e+02\n",
            " 5.540e+02 1.950e+02 1.370e+02 3.250e+02 2.350e+02 3.530e+02 3.310e+02\n",
            " 3.210e+02 1.640e+02 2.030e+02 2.500e+02 1.890e+02 2.480e+02 2.290e+02\n",
            " 4.620e+02 6.150e+02 2.120e+02 2.250e+02 3.990e+02 3.700e+02 3.260e+02\n",
            " 4.140e+02 2.690e+02 7.540e+02 2.540e+02 3.180e+02 3.610e+02 2.850e+02\n",
            " 3.040e+02 1.066e+03 2.160e+02 1.630e+02 2.000e+00 3.020e+02 4.480e+02\n",
            " 2.390e+02 2.790e+02 4.770e+02 2.820e+02 1.720e+02 2.610e+02 4.550e+02\n",
            " 5.740e+02 1.700e+02 2.280e+02 2.260e+02 4.260e+02 2.300e+02 2.890e+02\n",
            " 3.100e+02 3.930e+02 1.830e+02 3.010e+02 2.330e+02 3.600e+02 6.400e+02\n",
            " 2.360e+02 5.240e+02 4.300e+02 3.220e+02 1.620e+02 3.380e+02 4.210e+02\n",
            " 2.470e+02 1.870e+02 8.030e+02 3.820e+02 2.870e+02 1.850e+02 3.760e+02\n",
            " 1.920e+02 4.700e+02 1.910e+02 3.350e+02 7.600e+02 5.160e+02 3.960e+02\n",
            " 3.410e+02 5.800e+02 1.940e+02 1.970e+02 3.920e+02 2.720e+02 4.710e+02\n",
            " 2.230e+02 2.970e+02 5.310e+02 5.360e+02 5.220e+02 2.430e+02 3.400e+02\n",
            " 3.160e+02 4.490e+02 8.900e+02 3.330e+02 1.980e+02 6.030e+02 2.750e+02\n",
            " 2.020e+02 2.040e+02 4.270e+02 2.910e+02 3.800e+02 2.900e+02 2.190e+02\n",
            " 5.090e+02 6.270e+02 2.660e+02 2.420e+02 2.450e+02 4.910e+02 2.220e+02\n",
            " 4.070e+02 4.760e+02 8.760e+02 1.029e+03 4.170e+02 2.600e+02 5.650e+02\n",
            " 3.870e+02 2.980e+02 4.100e+02 6.000e+02 1.710e+02 9.990e+02 1.600e+03\n",
            " 4.220e+02 2.310e+02 3.830e+02 4.400e+02 2.810e+02 4.630e+02 3.280e+02\n",
            " 2.840e+02 3.440e+02 2.320e+02 3.110e+02 8.340e+02 5.000e+02 2.630e+02\n",
            " 4.450e+02 4.020e+02 2.440e+02 2.110e+02 2.930e+02 8.120e+02 1.686e+03\n",
            " 3.000e+02 1.436e+03 2.530e+02 3.650e+02 2.550e+02 9.440e+02 3.640e+02\n",
            " 4.230e+02 7.940e+02 3.390e+02 3.090e+02 3.720e+02 3.240e+02 7.000e+02\n",
            " 1.650e+03 6.780e+02 2.340e+02 6.090e+02 1.164e+03 3.060e+02 6.260e+02\n",
            " 2.710e+02 3.540e+02 6.120e+02 2.590e+02 5.620e+02 3.620e+02 3.740e+02\n",
            " 4.420e+02 6.870e+02 2.860e+02 2.070e+02 2.140e+02 4.320e+02 4.060e+02\n",
            " 3.520e+02 4.000e+02 2.940e+02 2.580e+02 4.740e+02 6.180e+02 2.670e+02\n",
            " 3.690e+02 4.240e+02 2.800e+02 5.640e+02 3.860e+02 3.300e+02 3.940e+02\n",
            " 4.160e+02 5.110e+02 3.470e+02 3.840e+02 3.580e+02 5.750e+02 2.960e+02\n",
            " 4.990e+02 3.980e+02 3.570e+02 2.560e+02 5.080e+02 4.870e+02 2.830e+02\n",
            " 7.780e+02 6.690e+02 5.680e+02 3.590e+02 7.290e+02 2.570e+02 3.510e+02\n",
            " 5.070e+02 3.290e+02 1.540e+03 4.860e+02 3.742e+03 3.030e+02 6.940e+02\n",
            " 3.680e+02 3.440e+03 3.080e+02 6.380e+02 9.999e+03 4.940e+02 4.340e+02\n",
            " 1.026e+03 3.120e+02 4.090e+02 3.050e+02 4.050e+02 1.454e+03 5.180e+02\n",
            " 4.560e+02 4.190e+02 4.670e+02 4.460e+02 2.270e+02 2.730e+02 6.550e+02\n",
            " 9.230e+02 1.240e+03 5.410e+02 5.130e+02 1.052e+03 2.380e+02 4.690e+02\n",
            " 3.450e+02 5.260e+02 4.370e+02 4.500e+02 5.550e+02 3.480e+02 2.490e+02\n",
            " 2.520e+02 4.750e+02 3.420e+02 7.340e+02 7.000e+03 5.590e+02 5.630e+02\n",
            " 4.180e+02 8.720e+02 3.430e+02 4.440e+02 3.235e+03 3.130e+02 2.650e+02\n",
            " 2.700e+02 6.640e+02 9.150e+02 1.377e+03 8.470e+02 3.270e+02 3.490e+02\n",
            " 8.600e+02 9.700e+02 3.660e+02 7.800e+02 3.200e+02 5.610e+02 7.500e+02\n",
            " 1.041e+03 8.670e+02 7.460e+02 6.220e+02 7.580e+02 7.720e+02 7.020e+02\n",
            " 4.590e+02 6.170e+02 4.110e+02 6.080e+02 9.790e+02 6.290e+02 5.210e+02\n",
            " 1.318e+03 6.420e+02 7.810e+02 2.760e+02 7.440e+02 1.509e+03 1.078e+03\n",
            " 8.930e+02 3.900e+02 5.790e+02 3.670e+02 3.770e+02 2.670e+03 5.140e+02\n",
            " 7.090e+02 4.970e+02 9.580e+02 3.150e+02 8.270e+02 6.430e+02 4.900e+02\n",
            " 3.500e+02 1.962e+03 3.190e+02 4.800e+02 5.190e+02 4.640e+02 6.240e+02\n",
            " 4.830e+02 5.500e+02 8.200e+02 6.360e+02 6.650e+02 9.880e+02 7.530e+02\n",
            " 3.360e+02]\n",
            "\n",
            "Unique values in 'SGOT_ALT':\n",
            "[3.500e+01 3.600e+01 3.200e+01 3.400e+01 1.200e+01 4.000e+01 1.800e+01\n",
            " 2.300e+01 3.800e+01 1.400e+01 5.100e+01 2.000e+01 1.600e+01 2.400e+01\n",
            " 1.000e+01 7.000e+00 1.700e+01 1.300e+01 3.000e+01 2.100e+01 3.100e+01\n",
            " 1.100e+01 2.900e+01 4.300e+01 1.500e+01 4.500e+01 4.600e+01 2.500e+01\n",
            " 1.900e+01 2.700e+01 4.200e+01 8.000e+00 5.200e+01 1.110e+02 3.700e+01\n",
            " 9.000e+00 6.000e+00 6.200e+01 5.600e+01 4.800e+01 3.300e+01 7.700e+01\n",
            " 2.600e+01 2.200e+01 5.000e+01 6.700e+01 2.800e+01 3.900e+01 5.500e+01\n",
            " 5.400e+01 6.400e+01 4.700e+01 6.000e+01 4.100e+01 7.500e+01 6.100e+01\n",
            " 9.000e+01 5.800e+01 1.060e+02 4.400e+01 9.600e+01 7.800e+01 1.670e+02\n",
            " 6.500e+01 6.300e+01 1.100e+02 4.900e+01 6.600e+01 9.200e+01 3.110e+02\n",
            " 5.300e+01 7.600e+01 5.900e+01 8.900e+01 2.360e+02 1.220e+02 8.800e+01\n",
            " 1.210e+02 1.450e+02 8.000e+01 5.700e+01 6.800e+01 9.700e+01 8.500e+01\n",
            " 5.000e+00 7.400e+01 8.700e+01 1.380e+02 6.900e+01 1.030e+02 1.930e+02\n",
            " 1.150e+02 1.240e+02 8.300e+01 2.270e+02 8.200e+01 7.000e+01 9.800e+01\n",
            " 8.100e+01 1.190e+02 7.300e+01 3.210e+02 1.010e+02 7.900e+01 1.070e+02\n",
            " 1.180e+02 7.100e+01 2.320e+02 9.500e+01 7.200e+01 1.120e+02 1.160e+02\n",
            " 1.610e+02 2.020e+02 1.040e+02 9.100e+01 1.000e+02 1.280e+02 2.070e+02\n",
            " 1.860e+02 1.520e+02 8.400e+01 1.090e+02 1.270e+02 8.600e+01 9.900e+01\n",
            " 1.260e+02 9.400e+01 4.000e+00 9.300e+01 1.310e+02 1.330e+02 1.050e+02\n",
            " 1.570e+02 4.300e+02 1.790e+02 1.870e+02 1.130e+02 1.530e+02 1.350e+02\n",
            " 1.020e+02 1.290e+02 2.100e+02 1.560e+02 1.640e+02 1.410e+02 1.140e+02\n",
            " 2.490e+02 1.250e+02 1.430e+02 1.340e+02 3.000e+00 1.590e+02 1.740e+02\n",
            " 1.480e+02 1.400e+02 1.080e+02 1.800e+02 1.650e+02 2.690e+02 2.530e+02\n",
            " 3.080e+02 1.830e+02 1.300e+02 1.700e+02 2.060e+02 2.000e+00 1.620e+02\n",
            " 1.200e+02 1.440e+02 1.460e+02 1.510e+02 1.810e+02 1.940e+02 1.980e+02\n",
            " 1.820e+02 1.550e+02 2.350e+02 1.680e+02 1.540e+02 1.950e+02 1.170e+02\n",
            " 1.600e+02 6.590e+02 2.650e+02 1.230e+02 3.150e+02 1.630e+02 1.320e+02\n",
            " 4.270e+02 2.130e+02 1.390e+02 4.490e+02 2.140e+02 2.390e+02 3.220e+02\n",
            " 1.500e+02 1.360e+02 2.180e+02 1.420e+02 1.910e+02 1.660e+02 1.490e+02\n",
            " 2.620e+02 1.970e+02 2.420e+02 2.050e+02 2.220e+02 2.309e+03 2.810e+02\n",
            " 2.150e+02 1.470e+02 2.300e+02 2.230e+02 2.710e+02 2.860e+02 3.170e+02\n",
            " 1.780e+02 3.980e+02 1.720e+02 2.950e+02 2.040e+02 2.450e+02 1.840e+02\n",
            " 2.640e+02 2.470e+02 3.650e+02 1.990e+02 3.000e+02 1.370e+02 3.200e+02\n",
            " 2.120e+02 2.090e+02 1.159e+03 1.920e+02 1.890e+02 4.960e+02 2.540e+02\n",
            " 1.580e+02 3.350e+02 1.330e+03 1.730e+02 2.960e+02 3.560e+02 2.340e+02\n",
            " 2.080e+03 2.190e+02 2.010e+02 3.660e+02 2.980e+02 3.120e+02 2.030e+02\n",
            " 3.070e+02 3.610e+02 4.430e+02 2.160e+02 3.300e+02 6.920e+02 1.770e+02\n",
            " 3.020e+02 1.000e+00 4.730e+02 1.710e+02 1.200e+03 1.850e+02 2.380e+02\n",
            " 2.890e+02 1.900e+02 1.750e+02 3.780e+02 1.880e+02 2.210e+02 8.380e+02\n",
            " 2.260e+02 2.610e+02 3.230e+02 2.830e+02 2.000e+02 2.560e+02 3.330e+02\n",
            " 3.240e+02 2.580e+02 2.480e+02 2.170e+02 3.590e+02 3.580e+02 2.550e+02\n",
            " 2.080e+02 4.980e+02 2.460e+02 3.030e+02 3.380e+02 1.760e+02 2.110e+02\n",
            " 6.400e+02 3.460e+02 2.840e+02 2.310e+02 2.590e+02 2.250e+02 3.130e+02\n",
            " 3.320e+02 3.340e+02 2.990e+02 3.820e+02 6.370e+02 2.430e+02 1.690e+02\n",
            " 9.130e+02 2.570e+02 3.540e+02 8.890e+02 3.900e+02 4.040e+02 3.360e+02\n",
            " 3.880e+02 3.180e+02 2.400e+02 5.410e+02 3.940e+02 2.200e+02 2.910e+02\n",
            " 3.250e+02 2.520e+02 6.100e+02 8.020e+02 2.850e+02 3.100e+02 3.040e+02\n",
            " 4.440e+02 2.290e+02 6.660e+02 3.290e+02 2.760e+02 5.670e+02 3.910e+02\n",
            " 2.770e+02 4.210e+02 2.440e+02 3.410e+02 3.140e+02 2.680e+02 3.270e+02\n",
            " 5.770e+02 4.470e+02 2.970e+02 1.960e+02 2.240e+02 4.810e+02 3.730e+02\n",
            " 4.330e+02 2.900e+02 4.570e+02 4.760e+02 2.730e+02 2.330e+02 3.770e+02\n",
            " 3.090e+02 5.890e+02 4.130e+02 3.310e+02 7.870e+02 2.940e+02 3.740e+02\n",
            " 4.070e+02 1.267e+03 2.280e+02 4.940e+02 3.160e+02 2.750e+02 4.690e+02\n",
            " 2.600e+02 5.200e+02 3.620e+02 2.700e+02 5.290e+02 5.610e+02 1.212e+03\n",
            " 5.280e+02 4.510e+02 3.990e+02 4.200e+02 5.310e+02 2.370e+02 5.240e+02\n",
            " 3.810e+02 8.290e+02 2.720e+02 4.160e+02 4.870e+02 6.870e+02 8.000e+02\n",
            " 3.260e+02 9.990e+02 8.300e+02 3.850e+02 6.900e+02 9.690e+02 3.860e+02\n",
            " 2.790e+02 4.900e+02 3.570e+02 4.370e+02 1.858e+03 4.860e+02 3.760e+02\n",
            " 3.430e+02 2.410e+02 3.470e+02 3.690e+02 4.380e+02 6.070e+02 4.540e+02\n",
            " 6.170e+02 1.274e+03 2.660e+02 2.880e+02 2.820e+02 3.520e+02 5.580e+02\n",
            " 2.740e+02 2.535e+03 1.940e+03 5.180e+02 3.970e+02 4.240e+02 3.600e+02\n",
            " 3.440e+02 4.600e+02 1.471e+03 2.630e+02 3.050e+02 5.510e+02 2.780e+02\n",
            " 2.500e+02 5.340e+02 4.010e+02 5.710e+02 1.221e+03 1.238e+03 5.990e+02\n",
            " 4.260e+02 5.830e+02 6.710e+02 2.030e+03 5.860e+02 9.810e+02 2.670e+02\n",
            " 3.750e+02 6.230e+02 5.690e+02 5.210e+02 2.930e+02 3.500e+02 5.570e+02\n",
            " 2.800e+02 2.510e+02 2.870e+02 3.720e+02 4.560e+02 7.020e+02 4.060e+02\n",
            " 3.920e+02 4.720e+02 7.440e+02 4.090e+02 4.450e+02 3.830e+02 4.790e+02\n",
            " 3.010e+02 3.630e+02 3.890e+02 1.016e+03 1.128e+03 7.490e+02 6.890e+02\n",
            " 6.860e+02 3.390e+02 2.088e+03 5.470e+02 4.250e+02 1.337e+03 4.950e+02\n",
            " 3.517e+03 3.450e+02 5.400e+02 3.800e+02 8.260e+02 3.550e+02 3.680e+02\n",
            " 5.420e+02 5.900e+02 4.190e+02 5.030e+02 2.920e+02 7.600e+02 8.330e+02\n",
            " 4.290e+02 3.840e+02 4.580e+02 6.160e+02 6.770e+02 3.490e+02 4.840e+02\n",
            " 1.336e+03 1.774e+03 1.351e+03 7.210e+03 1.052e+03 3.370e+02 1.830e+03\n",
            " 5.100e+02 9.860e+02 3.307e+03 4.830e+02 5.010e+02 1.390e+03 7.570e+02\n",
            " 4.880e+02 1.265e+03 4.050e+02 4.230e+02 5.880e+02 4.680e+02 3.060e+02\n",
            " 9.680e+02 9.750e+02 5.430e+02 4.120e+02 1.206e+03 3.400e+02 2.273e+03\n",
            " 5.460e+02 4.220e+02 2.530e+03 6.560e+02 6.610e+02 3.870e+02 7.220e+02\n",
            " 6.950e+02 8.740e+02 4.620e+02 4.180e+02 7.500e+02 5.370e+02 3.190e+02\n",
            " 5.680e+02 1.172e+03 4.310e+02 1.097e+03 7.000e+02 1.001e+03 9.100e+02\n",
            " 5.550e+02 9.380e+02 4.610e+02 1.216e+03 4.410e+02 3.807e+03 3.530e+02\n",
            " 4.633e+03 7.670e+02 7.070e+02 1.631e+03 5.110e+02 1.935e+03 5.740e+02\n",
            " 7.190e+02 2.981e+03 8.480e+02 8.370e+02 4.320e+02 5.160e+02 4.660e+02\n",
            " 6.880e+02 4.670e+02 5.060e+02 3.700e+02 1.522e+03 1.009e+03 4.150e+02\n",
            " 5.790e+02 4.590e+02 2.277e+03 2.698e+03 5.750e+02 4.080e+02 5.230e+02\n",
            " 7.390e+02 6.390e+02 4.000e+02 1.198e+03 4.640e+02 2.059e+03]\n",
            "\n",
            "Unique values in 'gamma_GTP':\n",
            "[ 40.  27.  68.  18.  25.  37.  12.  35.  26.  16.  19.  42.  31.  58.\n",
            "   9.  13.  60.  48.  32.  33.  15.  76.  29.  10.  53.  28.  45.  20.\n",
            "  62.  23.  93.  17.  22.  21. 278.  39.  14.  67.  49.  11. 292. 236.\n",
            "  47.  24.  52. 131.  83.  34.  50.  61. 160.  65.  36.  63.  38.  30.\n",
            " 151.  44.  86.  66. 179.   7. 180.  41.  77. 108.  70.  57.  56. 127.\n",
            " 119.  82.  46.  81.  43. 113. 122.  71. 143. 126.  64. 120.  55.  69.\n",
            "  51. 306.  72. 231. 107.  87.  59.   8. 256.   6. 189. 149.  98. 116.\n",
            " 133. 156.  54. 521.   3. 182. 111. 177.  73.  97.  95.  99. 513. 106.\n",
            " 196. 102.  75.  79. 125.   5. 176. 257.  96. 170.  78. 140.  74. 311.\n",
            "  90. 172. 150. 871.  84.  88. 538. 204.  92. 173. 123. 490. 186. 805.\n",
            " 193.  80. 146. 138.  94.  85. 389. 203. 147. 142. 100. 110. 145. 153.\n",
            " 284. 314. 117. 299. 999. 320. 217. 229. 188. 305. 429. 101. 178. 211.\n",
            " 260. 128.  89. 134. 184. 115. 112. 174. 192. 104. 195. 221. 287. 155.\n",
            " 187. 239. 171. 169. 562. 118. 162. 130. 161. 262. 342.  91. 105. 197.\n",
            " 270. 234. 255. 103. 166. 225. 167. 355. 226. 428. 144. 213. 374.   4.\n",
            " 109. 154. 165. 282. 148. 259. 212. 252. 599. 290. 273. 630. 201. 427.\n",
            " 157. 338. 420. 312. 202. 183. 137. 230. 359. 181. 617. 121. 208. 244.\n",
            " 425. 141. 424. 242. 310. 228. 158. 641. 296. 129. 135. 164. 248. 216.\n",
            " 506. 136. 356. 132. 307. 942. 330. 261. 124. 279. 280. 159. 316. 232.\n",
            " 286. 422. 210. 190. 114. 206. 249. 791. 467. 185. 139. 343. 549. 214.\n",
            " 411. 168. 459. 223. 313. 205. 269. 289. 209. 163. 265. 277. 472. 309.\n",
            " 175. 386. 317. 222. 655. 218. 328. 577. 250. 372. 663. 430. 293. 264.\n",
            " 453. 266. 198. 191. 345. 245. 194. 224. 240. 423. 272. 227. 247. 553.\n",
            " 349. 199. 294. 335. 449. 329. 847. 207. 325. 370. 518. 268. 322. 419.\n",
            " 377. 643. 387. 507. 488. 152. 624. 364. 283. 319. 455. 477. 291. 391.\n",
            " 402. 200. 215. 243. 220. 354. 281. 241. 341. 579. 253. 235. 333. 331.\n",
            " 725. 303. 451. 775. 486. 694. 501. 842. 297. 668. 401. 689. 740. 285.\n",
            " 434. 295. 251.   2. 497. 344. 371. 263. 326. 759. 276. 416. 574. 891.\n",
            " 323. 816. 233. 648. 763. 615. 246. 302. 839. 368. 304. 376. 569. 332.\n",
            " 366. 409. 408. 412. 605. 219. 405. 651. 822. 489. 667. 921. 375. 555.\n",
            " 275. 267. 727. 859. 595. 298. 348. 635. 352. 575. 481. 502. 421. 445.\n",
            " 308. 334. 498. 438. 444. 715. 439. 717. 433. 321. 760. 254. 380. 499.\n",
            " 873. 746. 340. 539. 597. 530. 336. 515. 454. 979. 739. 524. 565. 681.\n",
            " 476. 653. 926. 288. 889. 373. 768. 324. 972. 406. 959. 363. 383. 461.\n",
            " 882. 550. 450. 576. 779. 350. 384. 692. 675. 659. 353. 552. 743. 447.\n",
            " 991. 399. 351. 657. 656. 978. 690. 610. 367. 798. 339. 561. 726. 238.\n",
            " 388. 546. 396. 431. 361. 446. 478. 395. 456. 568. 381. 392. 315. 745.\n",
            " 619. 458. 394. 570. 369. 426. 540. 327. 758. 357. 795. 237. 931. 664.\n",
            " 258. 503. 713. 271. 385. 413. 403. 571. 770. 600. 469. 733. 442. 400.\n",
            " 390. 583. 362.   1. 300. 460. 517. 318. 346. 365. 418. 465. 410. 404.\n",
            " 415. 596. 301. 378. 878. 594. 677. 777. 676. 628. 468. 535. 407. 623.\n",
            " 925. 900. 817. 645. 441. 558. 393. 788. 436. 772. 971. 337. 586. 866.\n",
            " 622. 642. 753. 534. 274. 654. 732. 537. 850. 510. 443. 632. 905. 647.\n",
            " 955. 633. 720. 485. 998. 703. 749. 347. 483. 803. 382. 529. 572. 527.\n",
            " 686. 897. 693. 358. 432. 649. 578. 491. 731. 500. 508. 828. 474. 505.\n",
            " 774. 566. 943. 824. 683. 470. 601. 360. 493. 543. 701. 511. 606. 735.\n",
            " 836. 554. 820. 611. 646. 548. 564. 813. 519. 707. 964. 890. 588. 435.\n",
            " 616. 684. 464. 514. 522. 815. 448. 794. 638. 466. 977. 679. 864. 872.\n",
            " 523. 585. 902. 818. 457. 487. 750. 967. 714. 509. 709. 516. 800. 496.\n",
            " 398. 482. 479. 723. 604. 932. 492. 614. 580. 440. 437. 613. 924. 710.\n",
            " 660. 589. 417. 918. 495. 938. 629. 608. 716. 544. 940. 721. 782. 494.\n",
            " 662. 868. 769. 799. 463. 397. 625. 598. 678. 696. 894. 989. 591. 793.\n",
            " 379. 462. 823. 567. 851. 669. 560. 520. 841. 699. 612. 778. 947. 452.\n",
            " 846. 748. 626. 484. 950. 536. 551. 957. 867. 473. 414. 953. 920. 858.\n",
            " 547. 526. 865. 909. 786. 573. 528. 801. 584. 881. 888. 674. 831. 773.\n",
            " 545. 475. 874. 698. 837. 542. 914. 765. 992. 582. 532. 807. 639. 994.\n",
            " 908. 948. 658. 884. 541. 634. 910. 602. 590. 525. 747. 840. 670. 563.\n",
            " 833. 965. 827. 738. 756. 789. 640. 993. 533. 990. 937. 592. 904. 852.\n",
            " 621. 587. 819. 741. 650. 860. 593. 869. 951. 556. 697. 853. 618. 886.\n",
            " 711. 899. 906. 939. 688. 792. 712. 966. 806. 665. 512. 896. 752. 974.\n",
            " 734. 609. 581. 744. 861. 934. 809. 695. 927. 480. 504. 755. 718. 627.\n",
            " 702. 620. 700. 787. 907. 471. 781. 855. 811. 830. 736. 913. 751. 983.\n",
            " 704. 531. 705. 802. 814. 984. 559. 996. 728. 603. 875. 976. 637. 856.\n",
            " 919. 912. 652. 790. 729. 892. 825. 898. 935. 706. 607. 946. 783. 680.\n",
            " 761. 762. 970. 780. 986. 661. 785. 796. 771. 849. 933. 691. 687. 922.\n",
            " 557. 997. 848. 644. 730. 766. 742. 893. 666. 631. 724. 929. 672. 923.\n",
            " 903. 969. 737. 764. 754. 810. 835. 975. 911. 673. 917. 949. 821. 767.\n",
            " 636. 973.]\n",
            "\n",
            "Unique values in 'SMK_stat_type_cd':\n",
            "[1. 3. 2.]\n",
            "\n",
            "Unique values in 'DRK_YN':\n",
            "[1 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Untuk Model Smoker\n"
      ],
      "metadata": {
        "id": "DRAsrZUghBY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first 1000 columns from x\n",
        "x_smoker = x.iloc[:1000]\n",
        "\n",
        "# Select the first 1000 columns from y\n",
        "y_smoker = y.iloc[:1000]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_x_smoker, test_x_smoker, train_y_smoker, test_y_smoker = train_test_split(x_smoker, y_smoker, test_size=0.20, random_state=0)\n",
        "\n",
        "print(\"Train set:\", train_x_smoker.shape, train_y_smoker.shape)\n",
        "print(\"Test set:\", test_x_smoker.shape, test_y_smoker.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fUCsveRYbnW",
        "outputId": "385de186-c364-4ccd-f353-1ca2fd628ea4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (800, 22) (800, 1)\n",
            "Test set: (200, 22) (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation Random Forest\n",
        "train_y__smoker_flattened = train_y_smoker.values.ravel()\n",
        "model = RandomForestClassifier(n_estimators=600,random_state=1)\n",
        "model.fit(train_x_smoker, train_y__smoker_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Mf1vRBO_YdTj",
        "outputId": "9857dced-f25e-4975-baea-c1c3b3b04afd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=600, random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=600, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" checked><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=600, random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation Decision Tree\n",
        "model_tree = DecisionTreeClassifier(max_depth=2)\n",
        "model_tree.fit(train_x_smoker, train_y_smoker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "pjOSmRTTYq46",
        "outputId": "d88cf00b-b77c-41b8-af74-02703279fb18"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-20 {color: black;background-color: white;}#sk-container-id-20 pre{padding: 0;}#sk-container-id-20 div.sk-toggleable {background-color: white;}#sk-container-id-20 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-20 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-20 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-20 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-20 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-20 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-20 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-20 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-20 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-20 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-20 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-20 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-20 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-20 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-20 div.sk-item {position: relative;z-index: 1;}#sk-container-id-20 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-20 div.sk-item::before, #sk-container-id-20 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-20 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-20 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-20 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-20 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-20 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-20 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-20 div.sk-label-container {text-align: center;}#sk-container-id-20 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-20 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-20\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KNN model\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {'n_neighbors': np.arange(1, 11)}  # Adjust the range as needed\n",
        "\n",
        "# Use GridSearchCV to perform the grid search with 'f1_weighted' scoring\n",
        "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='f1_weighted')  # You can adjust the cross-validation folds (cv) as needed\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_x_smoker, train_y__smoker_flattened)\n",
        "\n",
        "# Print the best parameter(s) found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best cross-validated score\n",
        "print(\"Best cross-validated score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICTeLWpNY90H",
        "outputId": "9b63b9b5-cb62-4a7b-8b31-16fb45247d0c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_neighbors': 7}\n",
            "Best cross-validated score: 0.5555767704698401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=7)  # You can adjust the number of neighbors (k) as needed\n",
        "\n",
        "# Train the KNN model\n",
        "knn_model.fit(train_x_smoker, train_y_smoker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "4QAUw9h3ZAvH",
        "outputId": "8ad624a7-b735-478b-f795-99a2d8e4d766"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=7)"
            ],
            "text/html": [
              "<style>#sk-container-id-23 {color: black;background-color: white;}#sk-container-id-23 pre{padding: 0;}#sk-container-id-23 div.sk-toggleable {background-color: white;}#sk-container-id-23 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-23 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-23 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-23 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-23 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-23 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-23 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-23 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-23 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-23 div.sk-item {position: relative;z-index: 1;}#sk-container-id-23 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-23 div.sk-item::before, #sk-container-id-23 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-23 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-23 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-23 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-23 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-23 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-23 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-23 div.sk-label-container {text-align: center;}#sk-container-id-23 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-23 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=7)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation SVM\n",
        "model_svm = svm.SVC(kernel = 'rbf', probability =True)\n",
        "model_svm.fit(train_x_smoker, train_y_smoker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "4Iso9DQsZglY",
        "outputId": "3020cf25-4ef4-48e1-d3d5-5e1bb3921b15"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Untuk Model Drinkers"
      ],
      "metadata": {
        "id": "ZEdweXnOhJxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first 1000 columns from x\n",
        "x_drinker = x.iloc[:1000]\n",
        "\n",
        "# Select the first 1000 columns from y\n",
        "y2_drinker = y2.iloc[:1000]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_x_drinker, test_x_drinker, train_y_drinker, test_y_drinker = train_test_split(x_drinker, y2_drinker, test_size=0.20, random_state=0)\n",
        "\n",
        "print(\"Train set:\", train_x_drinker.shape, train_y_drinker.shape)\n",
        "print(\"Test set:\", test_x_drinker.shape, test_y_drinker.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH9KjeqOf-44",
        "outputId": "9ea3a16d-702d-448c-8274-7e635b79f175"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (800, 22) (800, 1)\n",
            "Test set: (200, 22) (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation Random Forest\n",
        "train_y__drinker_flattened = train_y_drinker.values.ravel()\n",
        "model = RandomForestClassifier(n_estimators=600,random_state=1)\n",
        "model.fit(train_x_drinker, train_y__drinker_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "pOzekO3egHrK",
        "outputId": "e4b4f3ce-9446-4bc2-a3ac-94aff6c6cbf7"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=600, random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=600, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=600, random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation Decision Tree\n",
        "model_tree = DecisionTreeClassifier(max_depth=2)\n",
        "model_tree.fit(train_x_drinker, train_y_drinker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "OW2OlwXfgMsG",
        "outputId": "98c581f9-ffdd-411e-9f1c-4e1fb1712118"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Generation SVM\n",
        "model_svm = svm.SVC(kernel = 'rbf', probability =True)\n",
        "model_svm.fit(train_x_drinker, train_y_drinker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "cj9GxjvOgP8k",
        "outputId": "4094ec45-7df7-4080-eb3a-08c0dfcf0377"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KNN model\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {'n_neighbors': np.arange(1, 11)}  # Adjust the range as needed\n",
        "\n",
        "# Use GridSearchCV to perform the grid search with 'f1_weighted' scoring\n",
        "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='f1_weighted')  # You can adjust the cross-validation folds (cv) as needed\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_x_drinker, train_y__drinker_flattened)\n",
        "\n",
        "# Print the best parameter(s) found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best cross-validated score\n",
        "print(\"Best cross-validated score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGtozHTFgpxz",
        "outputId": "18f65059-0315-42d5-db00-2824609b7363"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_neighbors': 9}\n",
            "Best cross-validated score: 0.6330360771750927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=9)  # You can adjust the number of neighbors (k) as needed\n",
        "\n",
        "# Train the KNN model\n",
        "knn_model.fit(train_x_drinker, train_y_drinker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "mIsGfGJ8gR19",
        "outputId": "cd93cf91-5acd-4a0a-c8c6-ce9ef1cb0cc6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=9)"
            ],
            "text/html": [
              "<style>#sk-container-id-27 {color: black;background-color: white;}#sk-container-id-27 pre{padding: 0;}#sk-container-id-27 div.sk-toggleable {background-color: white;}#sk-container-id-27 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-27 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-27 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-27 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-27 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-27 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-27 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-27 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-27 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-27 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-27 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-27 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-27 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-27 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-27 div.sk-item {position: relative;z-index: 1;}#sk-container-id-27 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-27 div.sk-item::before, #sk-container-id-27 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-27 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-27 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-27 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-27 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-27 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-27 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-27 div.sk-label-container {text-align: center;}#sk-container-id-27 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-27 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-27\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" checked><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=9)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}